{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BERT - Bidirectional Encoder Representations from Transformers - is a state-of-the-art natural language processing model that has produced impressive results in a variety of text-based tasks. However, it is a pre-trained model and often requires fine-tuning on specific tasks to achieve optimal performance.\n",
    "\n",
    "To fine-tune a BERT model on a special set of numerical data with random values missing, we'll need to follow these steps:\n",
    "\n",
    "1. Prepare the Data: We first need to prepare our data in a usable format for BERT. This involves converting our numerical dataset into a textual format that BERT can understand. For example, we can convert our input data to a tab-separated sequence of values, where the missing values are represented as `[MASK]`.\n",
    "\n",
    "2. Load the Pre-Trained Model: Next, we load a pre-trained BERT model that corresponds to our specific problem and task. There are several pre-trained BERT models available in the Hugging Face transformer library, such as bert-base-uncased, bert-large-uncased, and others.\n",
    "\n",
    "3. Fine-Tune the Model: After preparing our data and loading the pre-trained model, we can fine-tune the model on our specific task. This involves training the model on our numerical data while updating the weights of the pre-trained BERT model based on the gradients of our dataset. We typically use a GPU to speed up the process of fine-tuning the model, as it is a computationally expensive task.\n",
    "\n",
    "4. Evaluate the Model: Once we have fine-tuned our model, we evaluate its performance on a separate testing dataset. This allows us to verify the accuracy of our fine-tuned model and determine its suitability for our problem.\n",
    "\n",
    "Step 1: Prepare the data\n",
    "- Convert the numerical data to a text format that BERT can understand.\n",
    "- Randomly remove values from the data and replace them with `[MASK]` tokens.\n",
    "- Split the dataset into training and testing sets.\n",
    "\n",
    "Step 2: Load the pre-trained model\n",
    "- Import the pre-trained BERT model from the Hugging Face transformer library.\n",
    "- Define the hyperparameters for the model, such as the learning rate, batch size, and number of epochs.\n",
    "- Instantiate the model and load the pre-trained weights.\n",
    "\n",
    "Step 3: Fine-tune the model\n",
    "- Using the training dataset, fine-tune the pre-trained BERT model and update its weights.\n",
    "- Use a GPU to speed up the training process, if possible.\n",
    "- Monitor the loss and accuracy of the model during training to determine when to stop.\n",
    "\n",
    "Step 4: Evaluate the model\n",
    "- Use the testing dataset to evaluate the accuracy of the fine-tuned BERT model.\n",
    "- Use metrics such as accuracy, precision, recall, and F1 score to measure the performance of the model."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/ask/anaconda3/envs/STA208_BERT/bin/python\n",
      "py platform: macOS-13.4-arm64-arm-64bit\n",
      "python 3.9.16\n",
      "pandas 1.5.3\n",
      "numpy 1.24.3\n",
      "torch 2.0.1\n",
      "GPU is NOT available\n",
      "MPS is AVAILABLE\n",
      "Target device is mps\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import platform\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# torch.set_num_threads(2)\n",
    "\n",
    "from transformers import (\n",
    "    BertModel,\n",
    "    BertTokenizer,\n",
    "    TFBertForMaskedLM,\n",
    "    BertForMaskedLM,\n",
    "    AdamW,\n",
    "    get_linear_schedule_with_warmup,\n",
    ")\n",
    " \n",
    "\n",
    "    \n",
    "import sys\n",
    "print(sys.executable)\n",
    "print(f\"py platform: {platform.platform()}\") #Python Platform: macOS-13.3.1-arm64-arm-64bit\n",
    "from platform import python_version\n",
    "print('python ' + python_version())\n",
    "print(pd.__name__, pd.__version__)\n",
    "print(np.__name__, np.__version__)\n",
    "print(torch.__name__, torch.__version__)\n",
    "has_gpu = torch.cuda.is_available()\n",
    "has_mps = getattr(torch,'has_mps',False)\n",
    "device = \"mps\" if has_mps \\\n",
    "    else \"gpu\" if has_gpu else \"cpu\"\n",
    "\n",
    "print(\"GPU is\", \"AVAILABLE\" if has_gpu else \"NOT available\")#GPU is NOT AVAILABLE\n",
    "print(\"MPS is\", \"AVAILABLE\" if has_mps else \"NOT available\") #MPS is AVAILABLE\n",
    " \n",
    "print(f\"Target device is {device}\") #Target device is mps"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Processing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading All Training/Testing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.describe of          id        author geometry pressure [MPa] mass_flux [kg/m2-s]  \\\n",
       "0         0      Thompson     tube            7.0              3770.0   \n",
       "1         1      Thompson     tube            nan              6049.0   \n",
       "2         2      Thompson      nan          13.79              2034.0   \n",
       "3         3          Beus  annulus          13.79              3679.0   \n",
       "4         5           nan      nan          17.24              3648.0   \n",
       "...     ...           ...      ...            ...                 ...   \n",
       "23089  1861  Richenderfer    plate           1.01              1500.0   \n",
       "23090  1862  Richenderfer    plate           1.01              1500.0   \n",
       "23091  1863  Richenderfer    plate           1.01              2000.0   \n",
       "23092  1864  Richenderfer    plate           1.01              2000.0   \n",
       "23093  1865  Richenderfer    plate           1.01              2000.0   \n",
       "\n",
       "      D_e [mm] D_h [mm] length [mm] chf_exp [MW/m2] x_e_out [-]  \\\n",
       "0          nan     10.8       432.0             3.6      0.1754   \n",
       "1         10.3     10.3       762.0             6.2     -0.0416   \n",
       "2          7.7      7.7       457.0             2.5      0.0335   \n",
       "3          5.6     15.2      2134.0             3.0     -0.0279   \n",
       "4          nan      1.9       696.0             3.6     -0.0711   \n",
       "...        ...      ...         ...             ...         ...   \n",
       "23089     15.0    120.0        10.0             9.4     -0.0218   \n",
       "23090     15.0    120.0        10.0            10.4     -0.0434   \n",
       "23091     15.0    120.0        10.0            10.8     -0.0109   \n",
       "23092     15.0    120.0        10.0            10.9     -0.0218   \n",
       "23093     15.0    120.0        10.0            11.5     -0.0434   \n",
       "\n",
       "                                                sequence  \n",
       "0      author: Thompson geometry: tube pressure [MPa]...  \n",
       "1      author: Thompson geometry: tube pressure [MPa]...  \n",
       "2      author: Thompson geometry: nan pressure [MPa]:...  \n",
       "3      author: Beus geometry: annulus pressure [MPa]:...  \n",
       "4      author: nan geometry: nan pressure [MPa]: 17.2...  \n",
       "...                                                  ...  \n",
       "23089  author: Richenderfer geometry: plate pressure ...  \n",
       "23090  author: Richenderfer geometry: plate pressure ...  \n",
       "23091  author: Richenderfer geometry: plate pressure ...  \n",
       "23092  author: Richenderfer geometry: plate pressure ...  \n",
       "23093  author: Richenderfer geometry: plate pressure ...  \n",
       "\n",
       "[23094 rows x 11 columns]>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the numerical data you want to train BERT on\n",
    "df1 = pd.read_csv(\"data.csv\")\n",
    "df2 = pd.read_csv('Data_CHF_Zhao_2020_ATE.csv')\n",
    "frames = [df1, df2] \n",
    "df = pd.concat(frames)\n",
    "\n",
    "# Define the name of the column that you want to move to the end of the DataFrame\n",
    "column_name = \"x_e_out [-]\"\n",
    "\n",
    "# Select the column and drop it from the DataFrame\n",
    "column_to_move = df[column_name]\n",
    "col = df.drop(column_name, axis=1, inplace=True)\n",
    "\n",
    "# Append the column back to the end of the DataFrame\n",
    "df[column_name] = column_to_move\n",
    "\n",
    "# select only rows where x_e out exists\n",
    "data = df[~df[\"x_e_out [-]\"].isna()]\n",
    "\n",
    "# start with a small data set for speed\n",
    "#data = data[0:100]\n",
    "data = data.reset_index(drop=True)\n",
    "\n",
    "# Convert numerical values to string format to match BERT input requirement\n",
    "data = data.astype(str)\n",
    "\n",
    "data[\"sequence\"] = \"\"\n",
    "\n",
    "# Concatenate all the values in a row into a single string using the column names\n",
    "# Iterate through rows and columns\n",
    "for index, row in data.iterrows():\n",
    "    string = \"\"\n",
    "    for column in data.columns:\n",
    "        if column == \"x_e_out [-]\": \n",
    "            # make a mask with 7 sequential tokens\n",
    "            string += column + \": \" + '[MASK]'*7 + \" \"\n",
    "            continue\n",
    "        if column == \"sequence\" or column == 'id':\n",
    "            continue\n",
    "        string += column + \": \" + str(row[column]) + \" \"\n",
    "    masked_string = string.strip()\n",
    "    data[\"sequence\"][index] = masked_string\n",
    "\n",
    "data.describe"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Torch Masked BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.128\n",
      "Epoch 2, Loss: 0.103\n",
      "Epoch 3, Loss: 0.107\n",
      "Epoch 4, Loss: 0.093\n",
      "Epoch 5, Loss: 0.083\n",
      "Epoch 6, Loss: 0.083\n",
      "Epoch 7, Loss: 0.086\n",
      "Epoch 8, Loss: 0.077\n",
      "Epoch 9, Loss: 0.112\n",
      "Epoch 10, Loss: 0.084\n",
      "Accuracy: 125.082\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Instantiate the tokenizer, and attention-mask the tokens\n",
    "dropout_rate = 0.3\n",
    "MAX_LENGTH = 128\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertForMaskedLM.from_pretrained(\"bert-base-uncased\",\n",
    "                                        return_dict=True,\n",
    "                                        pad_token_id = 0\n",
    "                                        ).to('mps')\n",
    "model.train()\n",
    "\n",
    "sequences = data[\"sequence\"]\n",
    "x_e_out = data['x_e_out [-]']\n",
    "\n",
    "\n",
    "token_encoder_fun = lambda row: tokenizer.encode(row,\n",
    "                                                 add_special_tokens=True,\n",
    "                                                 padding=\"max_length\",\n",
    "                                                 #return_tensors='pt',\n",
    "                                                 max_length=MAX_LENGTH,\n",
    "                                                 )\n",
    "\n",
    "tokenized_data = sequences.apply(token_encoder_fun)\n",
    "tokenized_data = tokenized_data.reset_index(drop=True)\n",
    "\n",
    "tokenized_labels = x_e_out.apply(token_encoder_fun)\n",
    "tokenized_labels = tokenized_labels.reset_index(drop=True)\n",
    "\n",
    "input_ids = torch.tensor(tokenized_data)\n",
    "labels = torch.tensor(tokenized_labels)\n",
    "\n",
    "attention_masks = torch.empty( ( len(input_ids), MAX_LENGTH ) )\n",
    "\n",
    "for i in range(len(input_ids)):\n",
    "    tokens = input_ids[i, :]\n",
    "    row_mask = [int(token_id.item() > 0) for token_id in tokens]\n",
    "    row_mask = torch.tensor(row_mask).unsqueeze(0)\n",
    "    attention_masks[i] = row_mask\n",
    "    \n",
    "generator = torch.Generator().manual_seed(42)\n",
    "\n",
    "from torch.utils.data import TensorDataset, random_split\n",
    "\n",
    "# Combine the training inputs into a TensorDataset.\n",
    "dataset = TensorDataset(input_ids, attention_masks, labels)\n",
    "\n",
    "# Create a 90-10 train-validation split.\n",
    "\n",
    "# Calculate the number of samples to include in each set.\n",
    "train_size = int(0.9 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "\n",
    "# Divide the dataset by randomly selecting samples.\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "# The DataLoader needs to know our batch size for training. \n",
    "# For fine-tuning BERT on a specific task, the authors recommend a batch \n",
    "# size of 16 or 32.\n",
    "batch_size = 32\n",
    "\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "# Create the DataLoaders for our training and validation sets.\n",
    "# We'll take training samples in random order. \n",
    "train_dataloader = DataLoader(\n",
    "            train_dataset,  # The training samples.\n",
    "            sampler = RandomSampler(train_dataset), # Select batches randomly\n",
    "            batch_size = batch_size # Trains with this batch size.\n",
    "        )\n",
    "\n",
    "# For validation the order doesn't matter, so we'll just read them sequentially.\n",
    "test_dataloader = DataLoader(\n",
    "            test_dataset, # The validation samples.\n",
    "            sampler = SequentialSampler(test_dataset), # Pull out batches sequentially.\n",
    "            batch_size = batch_size # Evaluate with this batch size.\n",
    "        )\n",
    "# Step 2: Define the masked language modeling (MLM) loss function\n",
    "mask_token_id = tokenizer.mask_token_id\n",
    "\n",
    "# Step 3: Define the optimizer and loss fn\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Step 4: Train the model\n",
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in train_dataloader:\n",
    "        input_ids, attention_mask, labels = [x.to(device) for x in batch]\n",
    "\n",
    "        # Reset gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(input_ids, \n",
    "                        attention_mask=attention_mask, \n",
    "                        labels=labels)\n",
    "        loss = outputs.loss\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # Update the weights\n",
    "        optimizer.step()\n",
    "\n",
    "    # Print the loss and accuracy of the model for this epoch\n",
    "    print(f\"Epoch {epoch+1}, Loss: {loss:.3f}\")\n",
    "\n",
    "# Set model to evaluate mode\n",
    "model.eval()\n",
    "\n",
    "# Evaluate the model on the testing dataset\n",
    "accuracy = 0\n",
    "with torch.no_grad():\n",
    "  for batch in test_dataloader:\n",
    "    input_ids, attention_mask, labels = [x.to(device) for x in batch]\n",
    "\n",
    "    # Get the logits for the masked tokens\n",
    "    #outputs = model(input_ids, attention_mask=attention_mask)\n",
    "    outputs = model(input_ids, \n",
    "                    attention_mask=attention_mask, \n",
    "                    labels=labels)\n",
    "    logits = outputs.logits\n",
    "\n",
    "    # Get the predictions for the masked tokens\n",
    "    _, predictions = torch.max(logits, dim=2)\n",
    "\n",
    "    # Calculate the accuracy of the model on this batch\n",
    "    accuracy += np.sum(predictions.cpu().numpy() == labels.cpu().numpy())\n",
    "\n",
    "# Print the accuracy of the model\n",
    "print(f\"Accuracy: {accuracy / test_size:.3f}\")\n",
    "\n",
    "torch.save(model, 'bert_fine-tuned_3.sav')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "90/10 train/test split\n",
    "- `bert_fine-tuned_1.sav`\n",
    "  - epochs=2, training: data.csv\n",
    "#\n",
    "- `bert_fine-tuned_2.sav`\n",
    "  - epochs=2, training: data.csv + Data_CHF\n",
    "#\n",
    "- `bert_fine-tuned_3.sav`\n",
    "  - epochs=10, training: data + Data_CHF"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Appendix"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Old Snippets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.save(model, 'path/to/model')\n",
    "\n",
    "#saved_model = torch.load('path/to/model')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Console/Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode([101,1014,1014,1012,5840,102,102])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

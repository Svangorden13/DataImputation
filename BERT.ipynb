{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BERT - Bidirectional Encoder Representations from Transformers - is a state-of-the-art natural language processing model that has produced impressive results in a variety of text-based tasks. However, it is a pre-trained model and often requires fine-tuning on specific tasks to achieve optimal performance.\n",
    "\n",
    "To fine-tune a BERT model on a special set of numerical data with random values missing, we'll need to follow these steps:\n",
    "\n",
    "1. Prepare the Data: We first need to prepare our data in a usable format for BERT. This involves converting our numerical dataset into a textual format that BERT can understand. For example, we can convert our input data to a tab-separated sequence of values, where the missing values are represented as `[MASK]`.\n",
    "\n",
    "2. Load the Pre-Trained Model: Next, we load a pre-trained BERT model that corresponds to our specific problem and task. There are several pre-trained BERT models available in the Hugging Face transformer library, such as bert-base-uncased, bert-large-uncased, and others.\n",
    "\n",
    "3. Fine-Tune the Model: After preparing our data and loading the pre-trained model, we can fine-tune the model on our specific task. This involves training the model on our numerical data while updating the weights of the pre-trained BERT model based on the gradients of our dataset. We typically use a GPU to speed up the process of fine-tuning the model, as it is a computationally expensive task.\n",
    "\n",
    "4. Evaluate the Model: Once we have fine-tuned our model, we evaluate its performance on a separate testing dataset. This allows us to verify the accuracy of our fine-tuned model and determine its suitability for our problem.\n",
    "\n",
    "Step 1: Prepare the data\n",
    "- Convert the numerical data to a text format that BERT can understand.\n",
    "- Randomly remove values from the data and replace them with `[MASK]` tokens.\n",
    "- Split the dataset into training and testing sets.\n",
    "\n",
    "Step 2: Load the pre-trained model\n",
    "- Import the pre-trained BERT model from the Hugging Face transformer library.\n",
    "- Define the hyperparameters for the model, such as the learning rate, batch size, and number of epochs.\n",
    "- Instantiate the model and load the pre-trained weights.\n",
    "\n",
    "Step 3: Fine-tune the model\n",
    "- Using the training dataset, fine-tune the pre-trained BERT model and update its weights.\n",
    "- Use a GPU to speed up the training process, if possible.\n",
    "- Monitor the loss and accuracy of the model during training to determine when to stop.\n",
    "\n",
    "Step 4: Evaluate the model\n",
    "- Use the testing dataset to evaluate the accuracy of the fine-tuned BERT model.\n",
    "- Use metrics such as accuracy, precision, recall, and F1 score to measure the performance of the model."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ask/anaconda3/envs/STA208_BERT/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/ask/anaconda3/envs/STA208_BERT/bin/python\n",
      "python 3.9.16\n",
      "pandas 1.5.3\n",
      "numpy 1.24.3\n",
      "torch 1.13.1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# torch.set_num_threads(2)\n",
    "\n",
    "from transformers import (\n",
    "    BertModel,\n",
    "    BertTokenizer,\n",
    "    TFBertForMaskedLM,\n",
    "    BertForMaskedLM,\n",
    "    AdamW,\n",
    "    get_linear_schedule_with_warmup,\n",
    ")\n",
    "\n",
    "import sys\n",
    "print(sys.executable)\n",
    "from platform import python_version\n",
    "print('python ' + python_version())\n",
    "print(pd.__name__, pd.__version__)\n",
    "print(np.__name__, np.__version__)\n",
    "print(torch.__name__, torch.__version__)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Processing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading All Training/Testing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.describe of      id        author geometry pressure [MPa] mass_flux [kg/m2-s] D_e [mm]  \\\n",
       "0     0      Thompson     tube            7.0              3770.0      nan   \n",
       "1     1      Thompson     tube            nan              6049.0     10.3   \n",
       "2     2      Thompson      nan          13.79              2034.0      7.7   \n",
       "3     3          Beus  annulus          13.79              3679.0      5.6   \n",
       "4     5           nan      nan          17.24              3648.0      nan   \n",
       "..  ...           ...      ...            ...                 ...      ...   \n",
       "95  130      Thompson     tube          13.79              1356.0      7.8   \n",
       "96  131      Thompson      nan           3.45              3838.0     10.3   \n",
       "97  132           nan     tube          18.27              2197.0      3.0   \n",
       "98  133  Richenderfer    plate            0.2              5600.0      nan   \n",
       "99  134      Thompson     tube          18.96              3458.0      1.9   \n",
       "\n",
       "   D_h [mm] length [mm] chf_exp [MW/m2] x_e_out [-]  \\\n",
       "0      10.8       432.0             3.6      0.1754   \n",
       "1      10.3       762.0             6.2     -0.0416   \n",
       "2       7.7       457.0             2.5      0.0335   \n",
       "3      15.2      2134.0             3.0     -0.0279   \n",
       "4       1.9       696.0             3.6     -0.0711   \n",
       "..      ...         ...             ...         ...   \n",
       "95      7.8       591.0             2.7      0.0685   \n",
       "96     10.3       762.0             4.3      0.0051   \n",
       "97      3.0       150.0             3.2     -0.1258   \n",
       "98    120.0        10.0             5.2     -0.0373   \n",
       "99      1.9       152.0             6.6     -0.1209   \n",
       "\n",
       "                                             sequence  \n",
       "0   author: Thompson geometry: tube pressure [MPa]...  \n",
       "1   author: Thompson geometry: tube pressure [MPa]...  \n",
       "2   author: Thompson geometry: nan pressure [MPa]:...  \n",
       "3   author: Beus geometry: annulus pressure [MPa]:...  \n",
       "4   author: nan geometry: nan pressure [MPa]: 17.2...  \n",
       "..                                                ...  \n",
       "95  author: Thompson geometry: tube pressure [MPa]...  \n",
       "96  author: Thompson geometry: nan pressure [MPa]:...  \n",
       "97  author: nan geometry: tube pressure [MPa]: 18....  \n",
       "98  author: Richenderfer geometry: plate pressure ...  \n",
       "99  author: Thompson geometry: tube pressure [MPa]...  \n",
       "\n",
       "[100 rows x 11 columns]>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the numerical data you want to train BERT on\n",
    "df = pd.read_csv(\"data.csv\")\n",
    "\n",
    "# Define the name of the column that you want to move to the end of the DataFrame\n",
    "column_name = \"x_e_out [-]\"\n",
    "\n",
    "# Select the column and drop it from the DataFrame\n",
    "column_to_move = df[column_name]\n",
    "col = df.drop(column_name, axis=1, inplace=True)\n",
    "\n",
    "# Append the column back to the end of the DataFrame\n",
    "df[column_name] = column_to_move\n",
    "\n",
    "# select only rows where x_e out exists\n",
    "data = df[~df[\"x_e_out [-]\"].isna()]\n",
    "\n",
    "# start with a small data set for speed\n",
    "data = data[0:100]\n",
    "data = data.reset_index(drop=True)\n",
    "\n",
    "# Convert numerical values to string format to match BERT input requirement\n",
    "data = data.astype(str)\n",
    "\n",
    "data[\"sequence\"] = \"\"\n",
    "\n",
    "# Concatenate all the values in a row into a single string using the column names\n",
    "# Iterate through rows and columns\n",
    "for index, row in data.iterrows():\n",
    "    string = \"\"\n",
    "    for column in data.columns:\n",
    "        if column == \"x_e_out [-]\": \n",
    "            string += column + \": \" + '[MASK]' + \" \"\n",
    "            continue\n",
    "        if column == \"sequence\" or column == 'id':\n",
    "            continue\n",
    "        string += column + \": \" + str(row[column]) + \" \"\n",
    "    masked_string = string.strip()\n",
    "    data[\"sequence\"][index] = masked_string\n",
    "\n",
    "data.describe"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Torch Masked BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "can't convert np.ndarray of type numpy.object_. The only supported types are: float64, float32, float16, complex64, complex128, int64, int32, int16, int8, uint8, and bool.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 38\u001b[0m\n\u001b[1;32m     30\u001b[0m generator \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mGenerator()\u001b[39m.\u001b[39mmanual_seed(\u001b[39m42\u001b[39m)\n\u001b[1;32m     32\u001b[0m X_train, X_test, y_train, y_test \u001b[39m=\u001b[39m train_test_split(tokenized_data,\n\u001b[1;32m     33\u001b[0m                                                     tokenized_labels,\n\u001b[1;32m     34\u001b[0m                                                     test_size\u001b[39m=\u001b[39m\u001b[39m0.1\u001b[39m,\n\u001b[1;32m     35\u001b[0m                                                     random_state\u001b[39m=\u001b[39m\u001b[39m42\u001b[39m)\n\u001b[0;32m---> 38\u001b[0m X_train \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mtensor(X_train\u001b[39m.\u001b[39;49mvalues)\n\u001b[1;32m     39\u001b[0m y_train \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(y_train\u001b[39m.\u001b[39mvalues)\n\u001b[1;32m     40\u001b[0m X_test \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(X_test\u001b[39m.\u001b[39mvalues)\n",
      "\u001b[0;31mTypeError\u001b[0m: can't convert np.ndarray of type numpy.object_. The only supported types are: float64, float32, float16, complex64, complex128, int64, int32, int16, int8, uint8, and bool."
     ]
    }
   ],
   "source": [
    "# Step 1: Instantiate the tokenizer, and attention-mask the tokens\n",
    "dropout_rate = 0.3\n",
    "MAX_LENGTH = 128\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertForMaskedLM.from_pretrained(\"bert-base-uncased\",\n",
    "                                        return_dict=False,\n",
    "                                        pad_token_id = 0\n",
    "                                        )\n",
    "model.train()\n",
    "\n",
    "sequences = data[\"sequence\"]\n",
    "x_e_out = data['x_e_out [-]']\n",
    "\n",
    "token_encoder_fun = lambda row: tokenizer.encode(row,\n",
    "                                                 add_special_tokens=True,\n",
    "                                                 padding=\"max_length\",\n",
    "                                                 max_length=MAX_LENGTH\n",
    "                                                 )\n",
    "\n",
    "tokenized_data = sequences.apply(token_encoder_fun)\n",
    "tokenized_data = tokenized_data.reset_index(drop=True)\n",
    "\n",
    "tokenized_labels = x_e_out.apply(token_encoder_fun)\n",
    "tokenized_labels = tokenized_labels.reset_index(drop=True)\n",
    "\n",
    "input_data = torch.tensor(tokenized_data)\n",
    "labels = torch.tensor(tokenized_labels)\n",
    "\n",
    "generator = torch.Generator().manual_seed(42)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(tokenized_data,\n",
    "                                                    tokenized_labels,\n",
    "                                                    test_size=0.1,\n",
    "                                                    random_state=42)\n",
    "\n",
    "# stuck here, need to make these pd.Series -> torch.tensor\n",
    "X_train = torch.tensor(X_train.values)\n",
    "y_train = torch.tensor(y_train.values)\n",
    "X_test = torch.tensor(X_test.values)\n",
    "y_test = torch.tensor(y_test.values)\n",
    "\n",
    "# Create attention mask\n",
    "\"\"\" attention_mask = torch.empty( ( len(X_train), MAX_LENGTH ) )\n",
    "for i in range(len(X_train)):\n",
    "    input_ids = X_train[i, :]\n",
    "    row_mask = [int(token_id.item() > 0) for token_id in input_ids]\n",
    "    row_mask = torch.tensor(row_mask).unsqueeze(0)\n",
    "    attention_mask[i] = row_mask\n",
    " \"\"\"\n",
    "# Step 2: Define the masked language modeling (MLM) loss function\n",
    "mask_token_id = tokenizer.mask_token_id\n",
    "\n",
    "def mlm_loss(prediction_scores, labels):\n",
    "    masked_prediction_scores = torch.masked_select(prediction_scores, labels != mask_token_id )\n",
    "    masked_labels = torch.masked_select(labels, labels != mask_token_id)\n",
    "    return torch.nn.functional.cross_entropy(masked_prediction_scores, masked_labels)\n",
    "\n",
    "# Step 3: Define the optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\n",
    "\n",
    "# Step 4: Train the model\n",
    "num_epochs = 1\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for input_tok_seq, masked_ans_tok in zip(X_train, y_train):\n",
    "        outputs = model(input_ids=input_tok_seq, labels=y_test)\n",
    "        loss = outputs[0]\n",
    "        loss = mlm_loss(outputs, y_test)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "# Step 5: Evaluate the model\n",
    "model.eval()\n",
    "val_loader = (X_test, y_test)\n",
    "with torch.no_grad():\n",
    "    for input_ids,  in val_loader:\n",
    "        #prediction_scores = model(input_ids=input_ids, attention_mask=input_masks, token_type_ids=input_segments)[0]\n",
    "        loss = mlm_loss(prediction_scores, labels)\n",
    "        # do something with the predictions and labels (e.g. calculate accuracy)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "text input must of type `str` (single example), `List[str]` (batch or single pretokenized example) or `List[List[str]]` (batch of pretokenized examples).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m tokenizer \u001b[39m=\u001b[39m BertTokenizer\u001b[39m.\u001b[39mfrom_pretrained(\u001b[39m\"\u001b[39m\u001b[39mbert-base-uncased\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     10\u001b[0m \u001b[39m# Tokenize the data\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m tokenized_data \u001b[39m=\u001b[39m tokenizer(data)\n\u001b[1;32m     13\u001b[0m \u001b[39m# Create the masked-token BERT model\u001b[39;00m\n\u001b[1;32m     14\u001b[0m masked_model \u001b[39m=\u001b[39m BertModel(model\u001b[39m.\u001b[39mconfig, output_hidden_states\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/anaconda3/envs/STA208_BERT/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2548\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.__call__\u001b[0;34m(self, text, text_pair, text_target, text_pair_target, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2546\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_in_target_context_manager:\n\u001b[1;32m   2547\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_switch_to_input_mode()\n\u001b[0;32m-> 2548\u001b[0m     encodings \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_one(text\u001b[39m=\u001b[39;49mtext, text_pair\u001b[39m=\u001b[39;49mtext_pair, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mall_kwargs)\n\u001b[1;32m   2549\u001b[0m \u001b[39mif\u001b[39;00m text_target \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   2550\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_switch_to_target_mode()\n",
      "File \u001b[0;32m~/anaconda3/envs/STA208_BERT/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2606\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase._call_one\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2603\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m   2605\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m _is_valid_text_input(text):\n\u001b[0;32m-> 2606\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   2607\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mtext input must of type `str` (single example), `List[str]` (batch or single pretokenized example) \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2608\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mor `List[List[str]]` (batch of pretokenized examples).\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2609\u001b[0m     )\n\u001b[1;32m   2611\u001b[0m \u001b[39mif\u001b[39;00m text_pair \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m _is_valid_text_input(text_pair):\n\u001b[1;32m   2612\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   2613\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mtext input must of type `str` (single example), `List[str]` (batch or single pretokenized example) \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2614\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mor `List[List[str]]` (batch of pretokenized examples).\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2615\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: text input must of type `str` (single example), `List[str]` (batch or single pretokenized example) or `List[List[str]]` (batch of pretokenized examples)."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertModel, BertTokenizer\n",
    "\n",
    "# Load the BERT model\n",
    "#model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Load the BERT tokenizer\n",
    "#tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "'''\n",
    "# Create the optimizer\n",
    "optimizer = torch.optim.AdamW(masked_model.parameters(), lr=0.0001)\n",
    "\n",
    "# Train the model\n",
    "for epoch in range(10):\n",
    "    for batch in tokenized_data:\n",
    "        # Get the input and output tensors\n",
    "        input_ids = batch[\"input_ids\"]\n",
    "        attention_mask = batch[\"attention_mask\"]\n",
    "        labels = batch[\"labels\"]\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = masked_model(input_ids, attention_mask=attention_mask)\n",
    "'''\n",
    "        # Compute the loss\n",
    "        loss = outputs[0]\n",
    "\n",
    "        # Backpropagate the loss\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "# Save the model\n",
    "masked_model.save_pretrained(\"masked_bert_model\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertForMaskedLM\n",
    "from torch.nn import functional as F\n",
    "import torch\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = BertForMaskedLM.from_pretrained(\"bert-base-uncased\", return_dict=True)\n",
    "text = \"where are my \" + tokenizer.mask_token + \"?\"\n",
    "input = tokenizer.encode_plus(text, return_tensors=\"pt\")\n",
    "mask_index = torch.where(input[\"input_ids\"][0] == tokenizer.mask_token_id)\n",
    "output = model(**input)\n",
    "logits = output.logits\n",
    "softmax = F.softmax(logits, dim=-1)\n",
    "mask_word = softmax[0, mask_index, :]\n",
    "top_10 = torch.topk(mask_word, 10, dim=1)[1][0]\n",
    "for token in top_10:\n",
    "    word = tokenizer.decode([token])\n",
    "    new_sentence = text.replace(tokenizer.mask_token, word)\n",
    "    print(new_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/Users/ask/anaconda3/envs/STA208_BERT/lib/python3.9/site-packages/transformers/optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (1x30522 and 768x1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 44\u001b[0m\n\u001b[1;32m     38\u001b[0m input_ids \u001b[39m=\u001b[39m input_ids\u001b[39m.\u001b[39munsqueeze(\u001b[39m0\u001b[39m)\n\u001b[1;32m     40\u001b[0m \u001b[39m# input_ids[[0]][0][0].item()\u001b[39;00m\n\u001b[1;32m     41\u001b[0m \n\u001b[1;32m     42\u001b[0m \u001b[39m# attention_mask = torch.tensor(attention_mask).unsqueeze(0)\u001b[39;00m\n\u001b[0;32m---> 44\u001b[0m y_pred \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mforward(input_ids\u001b[39m=\u001b[39;49minput_ids, attention_mask\u001b[39m=\u001b[39;49mattention_mask)\n\u001b[1;32m     46\u001b[0m \u001b[39m# Compute loss\u001b[39;00m\n\u001b[1;32m     47\u001b[0m y_true \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor([\u001b[39mfloat\u001b[39m(labels[i])])\n",
      "Cell \u001b[0;32mIn[6], line 33\u001b[0m, in \u001b[0;36mBERTSequenceImputer.forward\u001b[0;34m(self, input_ids, attention_mask)\u001b[0m\n\u001b[1;32m     30\u001b[0m embedded_seq \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(embedded_seq[\u001b[39m0\u001b[39m])\n\u001b[1;32m     32\u001b[0m \u001b[39m# Predict the missing value (in our case the last token) using the linear layer\u001b[39;00m\n\u001b[0;32m---> 33\u001b[0m output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfc(embedded_seq[:, \u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m])\n\u001b[1;32m     34\u001b[0m \u001b[39mreturn\u001b[39;00m output\n",
      "File \u001b[0;32m~/anaconda3/envs/STA208_BERT/lib/python3.9/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/STA208_BERT/lib/python3.9/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (1x30522 and 768x1)"
     ]
    }
   ],
   "source": [
    "# Load the pretrained BERT model and tokenizer to convert data to tokenize-able format\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "# model = BertForMaskedLM.from_pretrained(\"bert-base-uncased\", return_dict=True)\n",
    "\n",
    "sequences = data[\"sequence\"]\n",
    "\n",
    "MAX_LENGTH = 128\n",
    "tokenized_data = sequences.apply(\n",
    "    (\n",
    "        lambda row: tokenizer.encode(\n",
    "            row, add_special_tokens=True, padding=\"max_length\", max_length=MAX_LENGTH\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "tokenized_data = tokenized_data.reset_index(drop=True)\n",
    "input_data = torch.tensor(tokenized_data)\n",
    "\n",
    "# Define the optimizer to be used to train the model\n",
    "dropout_rate = 0.3\n",
    "model = BERTSequenceImputer(dropout_rate=dropout_rate)\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5, eps=1e-8)\n",
    "\n",
    "# Train the model over a set number of epochs\n",
    "epochs = 4\n",
    "for epoch in range(epochs):\n",
    "    epoch_loss = 0\n",
    "    for i in range(len(input_data)):\n",
    "        # Reset gradients\n",
    "        model.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        input_ids = input_data[i, :]\n",
    "\n",
    "        attention_mask = [int(token_id.item() > 0) for token_id in input_ids]\n",
    "        attention_mask = torch.tensor(attention_mask).unsqueeze(0)\n",
    "\n",
    "        input_ids = input_ids.unsqueeze(0)\n",
    "\n",
    "        # input_ids[[0]][0][0].item()\n",
    "\n",
    "        # attention_mask = torch.tensor(attention_mask).unsqueeze(0)\n",
    "\n",
    "        y_pred = model.forward(input_ids=input_ids, attention_mask=attention_mask)\n",
    "#        model.forward()\n",
    "\n",
    "        # Compute loss\n",
    "        y_true = torch.tensor([float(labels[i])])\n",
    "        loss_func = torch.nn.BCEWithLogitsLoss()\n",
    "        loss = loss_func(y_pred.view(-1), y_true.view(-1))\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # Update weights\n",
    "        optimizer.step()\n",
    "\n",
    "    # Print loss per epoch\n",
    "    print(f\"Epoch: {epoch+1}, Loss: {epoch_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can't convert np.ndarray of type numpy.object_. The only supported types are: float64, float32, float16, complex64, complex128, int64, int32, int16, int8, uint8, and bool.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m a \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame(X_train) \n\u001b[0;32m----> 2\u001b[0m a \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mtensor(a\u001b[39m.\u001b[39;49mvalues)\n",
      "\u001b[0;31mTypeError\u001b[0m: can't convert np.ndarray of type numpy.object_. The only supported types are: float64, float32, float16, complex64, complex128, int64, int32, int16, int8, uint8, and bool."
     ]
    }
   ],
   "source": [
    "a = pd.DataFrame(X_train) \n",
    "a = torch.tensor(a.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Int64Index([18, 30, 73, 33, 90,  4, 76, 77, 12, 31, 55, 88, 26, 42, 69, 15, 40,\n",
       "            96,  9, 72, 11, 47, 85, 28, 93,  5, 66, 65, 35, 16, 49, 34,  7, 95,\n",
       "            27, 19, 81, 25, 62, 13, 24,  3, 17, 38,  8, 78,  6, 64, 36, 89, 56,\n",
       "            99, 54, 43, 50, 67, 46, 68, 61, 97, 79, 41, 58, 48, 98, 57, 75, 32,\n",
       "            94, 59, 63, 84, 37, 29,  1, 52, 21,  2, 23, 87, 91, 74, 86, 82, 20,\n",
       "            60, 71, 14, 92, 51],\n",
       "           dtype='int64')"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

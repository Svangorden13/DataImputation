{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.describe of           id       author geometry pressure [MPa] mass_flux [kg/m2-s]  \\\n",
       "3          3         Beus  annulus          13.79              3679.0   \n",
       "9          9       Peskov     tube           12.0              1617.0   \n",
       "15        15     Thompson     tube          12.07              3255.0   \n",
       "17        17     Thompson     tube          10.41              2563.0   \n",
       "30        30      Janssen  annulus           9.68              5615.0   \n",
       "...      ...          ...      ...            ...                 ...   \n",
       "31623  31623      Janssen  annulus           8.27              2731.0   \n",
       "31624  31624  Weatherhead     tube          13.79               963.0   \n",
       "31626  31626     Thompson     tube          15.51              2984.0   \n",
       "31627  31627     Thompson     tube           0.64              3282.0   \n",
       "31635  31635     Thompson     tube          17.24              2984.0   \n",
       "\n",
       "      x_e_out [-] D_e [mm] D_h [mm] length [mm] chf_exp [MW/m2]  \\\n",
       "3         -0.0279      5.6     15.2      2134.0             3.0   \n",
       "9          0.1228     10.0     10.0       520.0             2.2   \n",
       "15         0.0406      1.9      1.9       152.0             2.9   \n",
       "17         0.1092      4.6      4.6       229.0             6.8   \n",
       "30         0.0196     12.7     38.1       914.0             2.9   \n",
       "...           ...      ...      ...         ...             ...   \n",
       "31623      0.1601      5.0     13.3      2134.0             1.3   \n",
       "31624      0.1693      7.7      7.7       457.0             1.5   \n",
       "31626      0.0253      1.9      1.9       696.0             2.5   \n",
       "31627     -0.1224      3.0      3.0       100.0             7.1   \n",
       "31635     -0.0417      1.9      1.9       152.0             3.9   \n",
       "\n",
       "                                                sequence  \n",
       "3      id: 3 author: Beus geometry: annulus pressure ...  \n",
       "9      id: 9 author: Peskov geometry: tube pressure [...  \n",
       "15     id: 15 author: Thompson geometry: tube pressur...  \n",
       "17     id: 17 author: Thompson geometry: tube pressur...  \n",
       "30     id: 30 author: Janssen geometry: annulus press...  \n",
       "...                                                  ...  \n",
       "31623  id: 31623 author: Janssen geometry: annulus pr...  \n",
       "31624  id: 31624 author: Weatherhead geometry: tube p...  \n",
       "31626  id: 31626 author: Thompson geometry: tube pres...  \n",
       "31627  id: 31627 author: Thompson geometry: tube pres...  \n",
       "31635  id: 31635 author: Thompson geometry: tube pres...  \n",
       "\n",
       "[6538 rows x 11 columns]>"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importing the necessary libraries\n",
    "import torch\n",
    "from transformers import BertModel, BertTokenizer, AdamW, get_linear_schedule_with_warmup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the numerical data you want to train BERT on\n",
    "data = pd.read_csv('data.csv')\n",
    "data.dropna(inplace=True)\n",
    "\n",
    "# Convert numerical values to string format to match BERT input requirement\n",
    "data = data.astype(str)\n",
    "\n",
    "data['sequence'] = ''\n",
    "\n",
    "# Concatenate all the values in a row into a single string using the column names\n",
    "# Iterate through rows and columns\n",
    "for index, row in data.iterrows():\n",
    "    string = ''\n",
    "    for column in data.columns:\n",
    "        if column != 'sequence':\n",
    "            # Concatenate column name with row value\n",
    "            string += column + ': ' + str(row[column]) + ' '\n",
    "    data['sequence'][index] = string\n",
    "\n",
    "data.describe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the BERT classifier to be used to train the model\n",
    "class BERTClassifier(torch.nn.Module):\n",
    "    def __init__(self, dropout_rate=0.3):\n",
    "        super(BERTClassifier, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "        self.dropout = torch.nn.Dropout(dropout_rate)\n",
    "        self.linear = torch.nn.Linear(768, 1)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        _, pooled_out = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        dropout_out = self.dropout(pooled_out)\n",
    "        linear_out = self.linear(dropout_out)\n",
    "        return linear_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "could not determine the shape of object type 'BatchEncoding'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/qv/jw3_7klx5hv1tmq24wwxryy40000gn/T/ipykernel_51219/1022617691.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m#a = len(data['sequence'])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mtokenized_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenized_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0minput_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenized_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: could not determine the shape of object type 'BatchEncoding'"
     ]
    }
   ],
   "source": [
    "# Load the pretrained BERT model and tokenizer to convert data to tokenize-able format\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "MAX_LENGTH = 64\n",
    "# Use the tokenizer to convert the data into tokens and then into PyTorch tensor format\n",
    "# Get the maximum length of all the sentences to pad the shorter ones to match that format\n",
    "tokenized_data = data['sequence'].apply((lambda x: tokenizer.encode_plus(x, \n",
    "                                                                    add_special_tokens=True,\n",
    "                                                                    padding='longest')\n",
    "                                        )\n",
    "                                        )\n",
    "\n",
    "#a = len(data['sequence'])\n",
    "tokenized_data = tokenized_data.reset_index(drop=True)\n",
    "input_data = torch.tensor(tokenized_data)\n",
    "\n",
    "\n",
    "# Define the optimizer to be used to train the model\n",
    "dropout_rate = 0.3\n",
    "model = BERTClassifier(dropout_rate=dropout_rate)\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5, eps=1e-8)\n",
    "\n",
    "# Train the model over a set number of epochs\n",
    "epochs = 4\n",
    "for epoch in range(epochs):\n",
    "    epoch_loss = 0\n",
    "    for i in range(len(input_data)):\n",
    "        # Reset gradients\n",
    "        model.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        input_ids = input_data[i]\n",
    "        attention_mask = [int(token_id > 0) for token_id in input_ids]\n",
    "        attention_mask = torch.tensor(attention_mask)\n",
    "        y_pred = model.forward(input_ids.view(1,-1), attention_mask.view(1,-1))\n",
    "        \n",
    "        # Compute loss\n",
    "        y_true = torch.tensor([float(labels[i])])\n",
    "        loss_func = torch.nn.BCEWithLogitsLoss()\n",
    "        loss = loss_func(y_pred.view(1,-1), y_true.view(1,-1))\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update weights\n",
    "        optimizer.step()\n",
    "        \n",
    "    # Print loss per epoch \n",
    "    print(f'Epoch: {epoch+1}, Loss: {epoch_loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = ([tokenized_data])\n",
    "a = np.transpose(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = len(data['sequence'])\n",
    "b = data['sequence'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "92"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenized_data[100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

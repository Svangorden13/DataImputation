{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BERT - Bidirectional Encoder Representations from Transformers - is a state-of-the-art natural language processing model that has produced impressive results in a variety of text-based tasks. However, it is a pre-trained model and often requires fine-tuning on specific tasks to achieve optimal performance.\n",
    "\n",
    "To fine-tune a BERT model on a special set of numerical data with random values missing, we'll need to follow these steps:\n",
    "\n",
    "1. Prepare the Data: We first need to prepare our data in a usable format for BERT. This involves converting our numerical dataset into a textual format that BERT can understand. For example, we can convert our input data to a tab-separated sequence of values, where the missing values are represented as `[MASK]`.\n",
    "\n",
    "2. Load the Pre-Trained Model: Next, we load a pre-trained BERT model that corresponds to our specific problem and task. There are several pre-trained BERT models available in the Hugging Face transformer library, such as bert-base-uncased, bert-large-uncased, and others.\n",
    "\n",
    "3. Fine-Tune the Model: After preparing our data and loading the pre-trained model, we can fine-tune the model on our specific task. This involves training the model on our numerical data while updating the weights of the pre-trained BERT model based on the gradients of our dataset. We typically use a GPU to speed up the process of fine-tuning the model, as it is a computationally expensive task.\n",
    "\n",
    "4. Evaluate the Model: Once we have fine-tuned our model, we evaluate its performance on a separate testing dataset. This allows us to verify the accuracy of our fine-tuned model and determine its suitability for our problem.\n",
    "\n",
    "Step 1: Prepare the data\n",
    "- Convert the numerical data to a text format that BERT can understand.\n",
    "- Randomly remove values from the data and replace them with `[MASK]` tokens.\n",
    "- Split the dataset into training and testing sets.\n",
    "\n",
    "Step 2: Load the pre-trained model\n",
    "- Import the pre-trained BERT model from the Hugging Face transformer library.\n",
    "- Define the hyperparameters for the model, such as the learning rate, batch size, and number of epochs.\n",
    "- Instantiate the model and load the pre-trained weights.\n",
    "\n",
    "Step 3: Fine-tune the model\n",
    "- Using the training dataset, fine-tune the pre-trained BERT model and update its weights.\n",
    "- Use a GPU to speed up the training process, if possible.\n",
    "- Monitor the loss and accuracy of the model during training to determine when to stop.\n",
    "\n",
    "Step 4: Evaluate the model\n",
    "- Use the testing dataset to evaluate the accuracy of the fine-tuned BERT model.\n",
    "- Use metrics such as accuracy, precision, recall, and F1 score to measure the performance of the model."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/ask/anaconda3/envs/STA208_BERT/bin/python\n",
      "py platform: macOS-13.4-arm64-arm-64bit\n",
      "python 3.9.16\n",
      "pandas 1.5.3\n",
      "numpy 1.24.3\n",
      "torch 2.0.1\n",
      "GPU is NOT available\n",
      "MPS is AVAILABLE\n",
      "Target device is mps\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader, random_split, SequentialSampler\n",
    "\n",
    "import platform\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# torch.set_num_threads(2)\n",
    "\n",
    "from transformers import (\n",
    "    BertModel,\n",
    "    BertTokenizer,\n",
    "    TFBertForMaskedLM,\n",
    "    BertForMaskedLM,\n",
    "    AdamW,\n",
    "    get_linear_schedule_with_warmup,\n",
    ")\n",
    " \n",
    "import sys\n",
    "print(sys.executable)\n",
    "print(\"py platform: {}\".format(platform.platform())) #Python Platform: macOS-13.3.1-arm64-arm-64bit\n",
    "from platform import python_version\n",
    "print('python ' + python_version())\n",
    "print(pd.__name__, pd.__version__)\n",
    "print(np.__name__, np.__version__)\n",
    "print(torch.__name__, torch.__version__)\n",
    "has_gpu = torch.cuda.is_available()\n",
    "has_mps = getattr(torch,'has_mps',False)\n",
    "device = \"mps\" if has_mps \\\n",
    "    else \"gpu\" if has_gpu else \"cpu\"\n",
    "\n",
    "print(\"GPU is\", \"AVAILABLE\" if has_gpu else \"NOT available\")#GPU is NOT AVAILABLE\n",
    "print(\"MPS is\", \"AVAILABLE\" if has_mps else \"NOT available\") #MPS is AVAILABLE\n",
    " \n",
    "print(f\"Target device is {device}\") #Target device is mps"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Processing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Evaluation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.describe of           id        author geometry pressure [MPa] mass_flux [kg/m2-s]  \\\n",
       "0          4           nan     tube          13.79               686.0   \n",
       "1          7        Peskov     tube           18.0               750.0   \n",
       "2         10      Thompson     tube            nan                 nan   \n",
       "3         12      Thompson      nan           6.89              7500.0   \n",
       "4         23          Beus  annulus          15.51              1355.0   \n",
       "...      ...           ...      ...            ...                 ...   \n",
       "10410  31633      Thompson     tube          11.03                 nan   \n",
       "10411  31634  Richenderfer    plate           1.01              2000.0   \n",
       "10412  31637   Weatherhead     tube          13.79               688.0   \n",
       "10413  31640           nan      nan          13.79                 nan   \n",
       "10414  31642      Thompson     tube           6.89              3825.0   \n",
       "\n",
       "      D_e [mm] D_h [mm] length [mm] chf_exp [MW/m2] x_e_out [-]  \\\n",
       "0         11.1     11.1       457.0             2.8         nan   \n",
       "1         10.0     10.0      1650.0             2.2         nan   \n",
       "2          1.9      1.9       152.0             3.2         nan   \n",
       "3          nan     12.8      1930.0             4.8         nan   \n",
       "4          5.6     15.2      2134.0             2.1         nan   \n",
       "...        ...      ...         ...             ...         ...   \n",
       "10410     11.5     11.5         nan             2.0         nan   \n",
       "10411     15.0    120.0        10.0             6.2         nan   \n",
       "10412      nan     11.1       457.0             2.3         nan   \n",
       "10413      4.7      4.7         nan             3.9         nan   \n",
       "10414     23.6     23.6      1972.0             3.7         nan   \n",
       "\n",
       "                                                sequence  \n",
       "0      author: nan geometry: tube pressure [MPa]: 13....  \n",
       "1      author: Peskov geometry: tube pressure [MPa]: ...  \n",
       "2      author: Thompson geometry: tube pressure [MPa]...  \n",
       "3      author: Thompson geometry: nan pressure [MPa]:...  \n",
       "4      author: Beus geometry: annulus pressure [MPa]:...  \n",
       "...                                                  ...  \n",
       "10410  author: Thompson geometry: tube pressure [MPa]...  \n",
       "10411  author: Richenderfer geometry: plate pressure ...  \n",
       "10412  author: Weatherhead geometry: tube pressure [M...  \n",
       "10413  author: nan geometry: nan pressure [MPa]: 13.7...  \n",
       "10414  author: Thompson geometry: tube pressure [MPa]...  \n",
       "\n",
       "[10415 rows x 11 columns]>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the numerical data you want to train BERT on\n",
    "df1 = pd.read_csv(\"data.csv\")\n",
    "#df2 = pd.read_csv('Data_CHF_Zhao_2020_ATE.csv')\n",
    "frames = [df1]\n",
    "df = pd.concat(frames)\n",
    "\n",
    "# Define the name of the column that you want to move to the end of the DataFrame\n",
    "column_name = \"x_e_out [-]\"\n",
    "\n",
    "# Select the column and drop it from the DataFrame\n",
    "column_to_move = df[column_name]\n",
    "col = df.drop(column_name, axis=1, inplace=True)\n",
    "\n",
    "# Append the column back to the end of the DataFrame\n",
    "df[column_name] = column_to_move\n",
    "\n",
    "# select only rows where x_e out DOES exist\n",
    "data_test = df[df[\"x_e_out [-]\"].isna()]\n",
    "\n",
    "# start with a small data set for speed\n",
    "#data = data[0:100]\n",
    "data_test = data_test.reset_index(drop=True)\n",
    "\n",
    "# Convert numerical values to string format to match BERT input requirement\n",
    "data_test = data_test.astype(str)\n",
    "\n",
    "data_test[\"sequence\"] = \"\"\n",
    "\n",
    "# Concatenate all the values in a row into a single string using the column names\n",
    "# Iterate through rows and columns\n",
    "for index, row in data_test.iterrows():\n",
    "    string = \"\"\n",
    "    for column in data_test.columns:\n",
    "        if column == \"x_e_out [-]\": \n",
    "            # make a mask with 7 sequential tokens\n",
    "            string += column + \": \" + '[MASK]'*7 + \" \"\n",
    "            continue\n",
    "        if column == \"sequence\" or column == 'id':\n",
    "            continue\n",
    "        string += column + \": \" + str(row[column]) + \" \"\n",
    "    masked_string = string.strip()\n",
    "    data_test[\"sequence\"][index] = masked_string\n",
    "\n",
    "data_test.describe"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading All Training/Testing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the numerical data you want to train BERT on\n",
    "#df1 = pd.read_csv(\"data.csv\")\n",
    "df2 = pd.read_csv('Data_CHF_Zhao_2020_ATE.csv')\n",
    "frames = [df2]\n",
    "df = pd.concat(frames)\n",
    "\n",
    "# Define the name of the column that you want to move to the end of the DataFrame\n",
    "column_name = \"x_e_out [-]\"\n",
    "\n",
    "# Select the column and drop it from the DataFrame\n",
    "column_to_move = df[column_name]\n",
    "col = df.drop(column_name, axis=1, inplace=True)\n",
    "\n",
    "# Append the column back to the end of the DataFrame\n",
    "df[column_name] = column_to_move\n",
    "\n",
    "# select only rows where x_e out exists\n",
    "data = df[~df[\"x_e_out [-]\"].isna()]\n",
    "\n",
    "# start with a small data set for speed\n",
    "#data = data[0:100]\n",
    "data = data.reset_index(drop=True)\n",
    "\n",
    "# Convert numerical values to string format to match BERT input requirement\n",
    "data = data.astype(str)\n",
    "\n",
    "data[\"sequence\"] = \"\"\n",
    "\n",
    "# Concatenate all the values in a row into a single string using the column names\n",
    "# Iterate through rows and columns\n",
    "for index, row in data.iterrows():\n",
    "    string = \"\"\n",
    "    for column in data.columns:\n",
    "        if column == \"x_e_out [-]\": \n",
    "            # make a mask with 7 sequential tokens\n",
    "            string += column + \": \" + '[MASK]'*4 + \" \"\n",
    "            continue\n",
    "        if column == \"sequence\" or column == 'id':\n",
    "            continue\n",
    "        string += column + \": \" + str(row[column]) + \" \"\n",
    "    masked_string = string.strip()\n",
    "    data[\"sequence\"][index] = masked_string\n",
    "\n",
    "data.describe"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train/Optimize \n",
    "BERT Masked LM in PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Make the tokenizer\n",
    "MAX_LENGTH = 128\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "token_encoder_fun = lambda row: tokenizer.encode(row,\n",
    "                                                 add_special_tokens=True,\n",
    "                                                 padding=\"max_length\",\n",
    "                                                 #return_tensors='pt',\n",
    "                                                 max_length=MAX_LENGTH,\n",
    "                                                 )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BertForMaskedLM.from_pretrained(\"bert-base-uncased\",\n",
    "                                        return_dict=True,\n",
    "                                        pad_token_id = 0\n",
    "                                        ).to(device)\n",
    "\n",
    "model.train()\n",
    "\n",
    "sequences = data[\"sequence\"]\n",
    "x_e_out = data['x_e_out [-]']\n",
    "\n",
    "tokenized_data = sequences.apply(token_encoder_fun)\n",
    "tokenized_data = tokenized_data.reset_index(drop=True)\n",
    "\n",
    "tokenized_labels = x_e_out.apply(token_encoder_fun)\n",
    "tokenized_labels = tokenized_labels.reset_index(drop=True)\n",
    "\n",
    "input_ids = torch.tensor(tokenized_data)\n",
    "labels = torch.tensor(tokenized_labels)\n",
    "\n",
    "attention_masks = torch.empty( ( len(input_ids), MAX_LENGTH ) )\n",
    "\n",
    "# Generate attention masks\n",
    "for i in range(len(input_ids)):\n",
    "    tokens = input_ids[i, :]\n",
    "    row_mask = [int(token_id.item() > 0) for token_id in tokens]\n",
    "    row_mask = torch.tensor(row_mask).unsqueeze(0)\n",
    "    attention_masks[i] = row_mask\n",
    "    \n",
    "generator = torch.Generator().manual_seed(42)\n",
    "\n",
    "# Combine the training inputs into a TensorDataset.\n",
    "dataset = TensorDataset(input_ids, attention_masks, labels)\n",
    "\n",
    "# Create a 90-10 train-validation split.\n",
    "\n",
    "# Calculate the number of samples to include in each set.\n",
    "train_size = int(0.9 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "\n",
    "# Divide the dataset by randomly selecting samples.\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "# The DataLoader needs to know our batch size for training. \n",
    "# For fine-tuning BERT on a specific task, the authors recommend a batch \n",
    "# size of 16 or 32.\n",
    "batch_size = 32\n",
    "\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "# Create the DataLoaders for our training and validation sets.\n",
    "# We'll take training samples in random order. \n",
    "train_dataloader = DataLoader(\n",
    "            train_dataset,  # The training samples.\n",
    "            sampler = RandomSampler(train_dataset), # Select batches randomly\n",
    "            batch_size = batch_size # Trains with this batch size.\n",
    "        )\n",
    "\n",
    "# For validation the order doesn't matter, so we'll just read them sequentially.\n",
    "test_dataloader = DataLoader(\n",
    "            test_dataset, # The validation samples.\n",
    "            sampler = SequentialSampler(test_dataset), # Pull out batches sequentially.\n",
    "            batch_size = batch_size # Evaluate with this batch size.\n",
    "        )\n",
    "# Step 2: Define the masked language modeling (MLM) loss function\n",
    "mask_token_id = tokenizer.mask_token_id\n",
    "\n",
    "# Step 3: Define the optimizer and loss fn\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Step 4: Train the model\n",
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in train_dataloader:\n",
    "        input_ids, attention_mask, labels = [x.to(device) for x in batch]\n",
    "\n",
    "        # Reset gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(input_ids, \n",
    "                        attention_mask=attention_mask, \n",
    "                        labels=labels)\n",
    "        loss = outputs.loss\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # Update the weights\n",
    "        optimizer.step()\n",
    "\n",
    "    # Print the loss and accuracy of the model for this epoch\n",
    "    print(f\"Epoch {epoch+1}, Loss: {loss:.3f}\")\n",
    "\n",
    "# Set model to evaluate mode\n",
    "model.eval()\n",
    "\n",
    "# Evaluate the model on the testing dataset\n",
    "accuracy = 0\n",
    "with torch.no_grad():\n",
    "  for batch in test_dataloader:\n",
    "    input_ids, attention_mask, labels = [x.to(device) for x in batch]\n",
    "\n",
    "    # Get the logits for the masked tokens\n",
    "    #outputs = model(input_ids, attention_mask=attention_mask)\n",
    "    outputs = model(input_ids, \n",
    "                    attention_mask=attention_mask, \n",
    "                    labels=labels)\n",
    "    logits = outputs.logits\n",
    "\n",
    "    # Get the predictions for the masked tokens\n",
    "    _, predictions = torch.max(logits, dim=2)\n",
    "\n",
    "    # Calculate the accuracy of the model on this batch\n",
    "    accuracy += np.sum(predictions.cpu().numpy() == labels.cpu().numpy())\n",
    "\n",
    "# Print the accuracy of the model\n",
    "print(f\"Accuracy: {accuracy / test_size:.3f}\")\n",
    "torch.save(model, 'bert_fine-tuned_5.sav')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predict Using Fine-Tuned BERT"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "90/10 train/test split\n",
    "\n",
    "##### 7 Sequential `[MASK]` tokens\n",
    "- `bert_fine-tuned_1.sav`\n",
    "  - epochs=2, training: data.csv\n",
    "#\n",
    "- `bert_fine-tuned_2.sav`\n",
    "  - epochs=2, training: data.csv + Data_CHF\n",
    "#\n",
    "- `bert_fine-tuned_3.sav`\n",
    "  - epochs=10, training: data + Data_CHF\n",
    "#\n",
    "- `bert_fine-tuned_4.sav`\n",
    "  - epochs=5, training: Data_CHF\n",
    "#\n",
    "- `bert_fine-tuned_5.sav`\n",
    "  - epochs=10, training: Data_CHF\n",
    "\n",
    "##### 4 Sequential `[MASK]` tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_token_id = tokenizer.mask_token_id\n",
    "\n",
    "model_name = 'bert_fine-tuned_3.sav'\n",
    "model = torch.load(model_name).to('mps')\n",
    "\n",
    "sequences = data_test[\"sequence\"]\n",
    "x_e_out = data_test['x_e_out [-]']\n",
    "\n",
    "tokenized_data = sequences.apply(token_encoder_fun)\n",
    "tokenized_data = tokenized_data.reset_index(drop=True)\n",
    "\n",
    "input_ids = torch.tensor(tokenized_data)\n",
    "\n",
    "attention_masks = torch.empty( ( len(input_ids), MAX_LENGTH ) )\n",
    "\n",
    "for i in range(len(input_ids)):\n",
    "    tokens = input_ids[i, :]\n",
    "    row_mask = [int(token_id.item() > 0) for token_id in tokens]\n",
    "    row_mask = torch.tensor(row_mask).unsqueeze(0)\n",
    "    attention_masks[i] = row_mask\n",
    "\n",
    "# Combine the training inputs into a TensorDataset.\n",
    "dataset = TensorDataset(input_ids, attention_masks)\n",
    "\n",
    "# For validation the order doesn't matter, so we'll just read them sequentially.\n",
    "prediction_dataloader = DataLoader(\n",
    "            dataset, # The validation samples.\n",
    "            sampler = SequentialSampler(dataset), # Pull out batches sequentially.\n",
    "        )\n",
    "\n",
    "for batch in prediction_dataloader:\n",
    "    input_ids, attention_mask = [x.to(device) for x in batch]\n",
    "\n",
    "    # Get the logits for the masked tokens\n",
    "    #outputs = model(input_ids, attention_mask=attention_mask)\n",
    "    token_logits = model(input_ids, \n",
    "                    attention_mask=attention_mask).logits\n",
    "\n",
    "    # Find the location of [MASK] and extract its logits\n",
    "    mask_token_index = np.argwhere(input_ids.cpu().numpy() == mask_token_id)\n",
    "\n",
    "    # Empty list of decoded\n",
    "    s = []\n",
    "\n",
    "    # loop over mask indices and ID, replace tokens\n",
    "    for tok_idx in mask_token_index:\n",
    "        mask_token_logits = token_logits[0, tok_idx, :]\n",
    "    \n",
    "        top_tokens = np.argsort(-mask_token_logits.cpu().detach())[:6].tolist()\n",
    "        # Remove special tokens that don't contain human-relevant information ([CLS] and [SEP])\n",
    "        top_tokens = [tok for tok in top_tokens if all([tok!=0, tok !=101, tok !=102])]\n",
    "        \n",
    "        # Choose the most likely remaining token and replace masked token\n",
    "        input_ids[0][tok_idx[1]] = top_tokens[0][0]\n",
    "\n",
    "    # Trim PSD, CLS, SEP tokens    \n",
    "    trim_toks = [ t for t in input_ids[0] if all([t!=0, t !=101, t !=102]) ]\n",
    "\n",
    "    s = s.append(tokenizer.decode(trim_toks))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Appendix"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Old Snippets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.save(model, 'path/to/model')\n",
    "\n",
    "#saved_model = torch.load('path/to/model')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Console/Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ P A D ]\n",
      "# # 8\n",
      "2 5 9\n",
      "# # 3 3\n",
      "# # 7\n"
     ]
    }
   ],
   "source": [
    "for tok in top_tokens:\n",
    "    if tok != 101 and tok != 102:\n",
    "        print(tokenizer.decode(tok)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n"
     ]
    }
   ],
   "source": [
    "for i in mask_token_index:\n",
    "    print(i[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [ t for t in input_ids[0] if all([t!=0, t !=101, t !=102]) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'author : beus geometry : nan pressure [ mpa ] : 13. 79 mass _ flux [ kg / m2 - s ] : 2730. 0 d _ e [ mm ] : 5. 6 d _ h [ mm ] : 15. 2 length [ mm ] : 2134. 0 chf _ exp [ mw / m2 ] : 1. 6 x _ e _ out [ - ] : [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK]'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "101"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_tokens[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.describe of      id        author geometry pressure [MPa] mass_flux [kg/m2-s] D_e [mm]  \\\n",
       "0     0      Thompson     tube            7.0              3770.0      nan   \n",
       "1     1      Thompson     tube            nan              6049.0     10.3   \n",
       "2     2      Thompson      nan          13.79              2034.0      7.7   \n",
       "3     3          Beus  annulus          13.79              3679.0      5.6   \n",
       "4     5           nan      nan          17.24              3648.0      nan   \n",
       "..  ...           ...      ...            ...                 ...      ...   \n",
       "95  130      Thompson     tube          13.79              1356.0      7.8   \n",
       "96  131      Thompson      nan           3.45              3838.0     10.3   \n",
       "97  132           nan     tube          18.27              2197.0      3.0   \n",
       "98  133  Richenderfer    plate            0.2              5600.0      nan   \n",
       "99  134      Thompson     tube          18.96              3458.0      1.9   \n",
       "\n",
       "   D_h [mm] length [mm] chf_exp [MW/m2] x_e_out [-]  \\\n",
       "0      10.8       432.0             3.6      0.1754   \n",
       "1      10.3       762.0             6.2     -0.0416   \n",
       "2       7.7       457.0             2.5      0.0335   \n",
       "3      15.2      2134.0             3.0     -0.0279   \n",
       "4       1.9       696.0             3.6     -0.0711   \n",
       "..      ...         ...             ...         ...   \n",
       "95      7.8       591.0             2.7      0.0685   \n",
       "96     10.3       762.0             4.3      0.0051   \n",
       "97      3.0       150.0             3.2     -0.1258   \n",
       "98    120.0        10.0             5.2     -0.0373   \n",
       "99      1.9       152.0             6.6     -0.1209   \n",
       "\n",
       "                                             sequence  \n",
       "0   id: 0 author: Thompson geometry: tube pressure...  \n",
       "1   id: 1 author: Thompson geometry: tube pressure...  \n",
       "2   id: 2 author: Thompson geometry: nan pressure ...  \n",
       "3   id: 3 author: Beus geometry: annulus pressure ...  \n",
       "4   id: 5 author: nan geometry: nan pressure [MPa]...  \n",
       "..                                                ...  \n",
       "95  id: 130 author: Thompson geometry: tube pressu...  \n",
       "96  id: 131 author: Thompson geometry: nan pressur...  \n",
       "97  id: 132 author: nan geometry: tube pressure [M...  \n",
       "98  id: 133 author: Richenderfer geometry: plate p...  \n",
       "99  id: 134 author: Thompson geometry: tube pressu...  \n",
       "\n",
       "[100 rows x 11 columns]>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importing the necessary libraries\n",
    "import torch\n",
    "from transformers import (\n",
    "    BertModel,\n",
    "    BertTokenizer,\n",
    "    AdamW,\n",
    "    get_linear_schedule_with_warmup,\n",
    ")\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the numerical data you want to train BERT on\n",
    "df = pd.read_csv(\"data.csv\")\n",
    "\n",
    "# Select only rows where x_e_out not missing\n",
    "\n",
    "# Define the name of the column that you want to move to the end of the DataFrame\n",
    "column_name = \"x_e_out [-]\"\n",
    "\n",
    "# Select the column and drop it from the DataFrame\n",
    "column_to_move = df[column_name]\n",
    "col = df.drop(column_name, axis=1, inplace=True)\n",
    "\n",
    "# Append the column back to the end of the DataFrame\n",
    "df[column_name] = column_to_move\n",
    "\n",
    "data = df[~df[\"x_e_out [-]\"].isna()]\n",
    "\n",
    "# start with a small data set for speed\n",
    "data = data[0:100]\n",
    "data = data.reset_index(drop=True)\n",
    "\n",
    "# Convert numerical values to string format to match BERT input requirement\n",
    "data = data.astype(str)\n",
    "\n",
    "data[\"sequence\"] = \"\"\n",
    "\n",
    "# Concatenate all the values in a row into a single string using the column names\n",
    "# Iterate through rows and columns\n",
    "for index, row in data.iterrows():\n",
    "    string = \"\"\n",
    "    for column in data.columns:\n",
    "        if column != \"sequence\":\n",
    "            # Concatenate column name with row value\n",
    "            string += column + \": \" + str(row[column]) + \" \"\n",
    "    data[\"sequence\"][index] = string\n",
    "\n",
    "data.describe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"AdamW# Define the BERT classifier to be used to train the model\n",
    "class BERTClassifier(torch.nn.Module):\n",
    "    def __init__(self, dropout_rate=0.3):\n",
    "        super(BERTClassifier, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "        self.dropout = torch.nn.Dropout(dropout_rate)\n",
    "        self.linear = torch.nn.Linear(768, 1)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        _, pooled_out = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        dropout_out = self.dropout(pooled_out)\n",
    "        linear_out = self.linear(dropout_out)\n",
    "        return linear_out\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "# Define the BERT Classifier\n",
    "class BERTSequenceImputer(torch.nn.Module):\n",
    "    def __init__(self, freeze_BERT=True, dropout_rate=0.3):\n",
    "        super(BERTSequenceImputer, self).__init__()\n",
    "\n",
    "        # Load the BERT model and tokenizer\n",
    "        self.bert = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "        # Freeze the weights of the BERT model\n",
    "        if freeze_BERT:\n",
    "            for param in self.bert.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "        # Define the linear layer for classification\n",
    "        self.dropout = torch.nn.Dropout(dropout_rate)\n",
    "        self.fc = torch.nn.Linear(768, 1)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "\n",
    "        # Convert the attention mask and padded_inputs into device friendly tensors\n",
    "        # attention_mask = attention_mask.to(torch.device(\"cuda:0\"))\n",
    "        # padded_inputs = padded_inputs.to(torch.device(\"cuda:0\"))\n",
    "\n",
    "        # Use the BERT model to transform the sequence into an embedded format\n",
    "        embedded_seq, _ = self.bert(input_ids=input_ids, attention_mask=attention_mask)[\n",
    "            :2\n",
    "        ]\n",
    "        embedded_seq = self.dropout(embedded_seq)\n",
    "\n",
    "        # Predict the missing value (in our case the last token) using the linear layer\n",
    "        output = self.fc(embedded_seq[:, -1])\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/Users/ask/opt/anaconda3/lib/python3.8/site-packages/transformers/optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 2, got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/qv/jw3_7klx5hv1tmq24wwxryy40000gn/T/ipykernel_63692/2901448729.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0mattention_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m         \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0;31m# Compute loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/qv/jw3_7klx5hv1tmq24wwxryy40000gn/T/ipykernel_63692/3979667310.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0;31m# Use the BERT model to transform the sequence into an embedded format\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0membedded_seq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0membedded_seq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedded_seq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    972\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"You have to specify either input_ids or inputs_embeds\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    973\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 974\u001b[0;31m         \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    975\u001b[0m         \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0minput_ids\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0minputs_embeds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    976\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 2, got 1)"
     ]
    }
   ],
   "source": [
    "# Load the pretrained BERT model and tokenizer to convert data to tokenize-able format\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "# model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "sequences = data[\"sequence\"]\n",
    "\n",
    "MAX_LENGTH = 128\n",
    "tokenized_data = sequences.apply(\n",
    "    (\n",
    "        lambda row: tokenizer.encode(\n",
    "            row, add_special_tokens=True, padding=\"max_length\", max_length=MAX_LENGTH\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\"\"\"\n",
    "# Use the tokenizer to convert the data into tokens and then into PyTorch tensor format\n",
    "# Get the maximum length of all the sentences to pad the shorter ones to match that format\n",
    "tokenized_data = sequences.apply((lambda x: tokenizer.encode(x, \n",
    "                                                                    add_special_tokens=True,\n",
    "                                                                    padding='longest')\n",
    "                                        )\n",
    "                                        )\n",
    "\"\"\"\n",
    "# a = len(data['sequence'])\n",
    "\n",
    "tokenized_data = tokenized_data.reset_index(drop=True)\n",
    "input_data = torch.tensor(tokenized_data)\n",
    "\n",
    "# Define the optimizer to be used to train the model\n",
    "dropout_rate = 0.3\n",
    "model = BERTSequenceImputer(dropout_rate=dropout_rate)\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5, eps=1e-8)\n",
    "\n",
    "# Train the model over a set number of epochs\n",
    "epochs = 4\n",
    "for epoch in range(epochs):\n",
    "    epoch_loss = 0\n",
    "    for i in range(len(input_data)):\n",
    "        # Reset gradients\n",
    "        model.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        input_ids = input_data[i, :]\n",
    "        # input_ids = input_ids.unsqueeze(0)\n",
    "\n",
    "        attention_mask = [int(token_id.item() > 0) for token_id in input_ids]\n",
    "\n",
    "        # input_ids[[0]][0][0].item()\n",
    "\n",
    "        attention_mask = torch.tensor(attention_mask).unsqueeze(0)\n",
    "\n",
    "        y_pred = model.forward(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "        # Compute loss\n",
    "        y_true = torch.tensor([float(labels[i])])\n",
    "        loss_func = torch.nn.BCEWithLogitsLoss()\n",
    "        loss = loss_func(y_pred.view(-1), y_true.view(-1))\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # Update weights\n",
    "        optimizer.step()\n",
    "\n",
    "    # Print loss per epoch\n",
    "    print(f\"Epoch: {epoch+1}, Loss: {epoch_loss}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

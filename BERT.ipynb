{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BERT - Bidirectional Encoder Representations from Transformers - is a state-of-the-art natural language processing model that has produced impressive results in a variety of text-based tasks. However, it is a pre-trained model and often requires fine-tuning on specific tasks to achieve optimal performance.\n",
    "\n",
    "To fine-tune a BERT model on a special set of numerical data with random values missing, we'll need to follow these steps:\n",
    "\n",
    "1. Prepare the Data: We first need to prepare our data in a usable format for BERT. This involves converting our numerical dataset into a textual format that BERT can understand. For example, we can convert our input data to a tab-separated sequence of values, where the missing values are represented as `[MASK]`.\n",
    "\n",
    "2. Load the Pre-Trained Model: Next, we load a pre-trained BERT model that corresponds to our specific problem and task. There are several pre-trained BERT models available in the Hugging Face transformer library, such as bert-base-uncased, bert-large-uncased, and others.\n",
    "\n",
    "3. Fine-Tune the Model: After preparing our data and loading the pre-trained model, we can fine-tune the model on our specific task. This involves training the model on our numerical data while updating the weights of the pre-trained BERT model based on the gradients of our dataset. We typically use a GPU to speed up the process of fine-tuning the model, as it is a computationally expensive task.\n",
    "\n",
    "4. Evaluate the Model: Once we have fine-tuned our model, we evaluate its performance on a separate testing dataset. This allows us to verify the accuracy of our fine-tuned model and determine its suitability for our problem.\n",
    "\n",
    "Step 1: Prepare the data\n",
    "- Convert the numerical data to a text format that BERT can understand.\n",
    "- Randomly remove values from the data and replace them with `[MASK]` tokens.\n",
    "- Split the dataset into training and testing sets.\n",
    "\n",
    "Step 2: Load the pre-trained model\n",
    "- Import the pre-trained BERT model from the Hugging Face transformer library.\n",
    "- Define the hyperparameters for the model, such as the learning rate, batch size, and number of epochs.\n",
    "- Instantiate the model and load the pre-trained weights.\n",
    "\n",
    "Step 3: Fine-tune the model\n",
    "- Using the training dataset, fine-tune the pre-trained BERT model and update its weights.\n",
    "- Use a GPU to speed up the training process, if possible.\n",
    "- Monitor the loss and accuracy of the model during training to determine when to stop.\n",
    "\n",
    "Step 4: Evaluate the model\n",
    "- Use the testing dataset to evaluate the accuracy of the fine-tuned BERT model.\n",
    "- Use metrics such as accuracy, precision, recall, and F1 score to measure the performance of the model."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/ask/anaconda3/envs/STA208_BERT/bin/python\n",
      "platform: macOS-13.4-arm64-arm-64bit\n",
      "python 3.9.16\n",
      "pandas 1.5.3\n",
      "numpy 1.24.3\n",
      "torch 2.0.1\n",
      "GPU is NOT available\n",
      "MPS is AVAILABLE\n",
      "target device is mps\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader, random_split, SequentialSampler, RandomSampler\n",
    "\n",
    "#\n",
    "from tqdm.notebook import tqdm, trange\n",
    "\n",
    "from platform import python_version, platform\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# torch.set_num_threads(2)\n",
    "\n",
    "from transformers import (\n",
    "    BertModel,\n",
    "    BertTokenizer,\n",
    "    TFBertForMaskedLM,\n",
    "    BertForMaskedLM,\n",
    "    AdamW,\n",
    "    get_linear_schedule_with_warmup,\n",
    ")\n",
    " \n",
    "from sys import executable\n",
    "print(executable)\n",
    "print(\"platform: {}\".format(platform())) #Python Platform: macOS-13.3.1-arm64-arm-64bit\n",
    "print('python ' + python_version())\n",
    "print(pd.__name__, pd.__version__)\n",
    "print(np.__name__, np.__version__)\n",
    "print(torch.__name__, torch.__version__)\n",
    "has_gpu = torch.cuda.is_available()\n",
    "has_mps = getattr(torch,'has_mps',False)\n",
    "device = \"mps\" if has_mps \\\n",
    "    else \"gpu\" if has_gpu else \"cpu\"\n",
    "\n",
    "print(\"GPU is\", \"AVAILABLE\" if has_gpu else \"NOT available\")#GPU is NOT AVAILABLE\n",
    "print(\"MPS is\", \"AVAILABLE\" if has_mps else \"NOT available\") #MPS is AVAILABLE\n",
    " \n",
    "print(\"target device is {}\".format(device)) #Target device is mps"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Processing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Evaluation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' # Load the numerical data you want to train BERT on\\ndf1 = pd.read_csv(\"data.csv\")\\n#df2 = pd.read_csv(\\'Data_CHF_Zhao_2020_ATE.csv\\')\\nframes = [df1]\\ndf = pd.concat(frames)\\n\\n# Define the name of the column that you want to move to the end of the DataFrame\\ncolumn_name = \"x_e_out [-]\"\\n\\n# Select the column and drop it from the DataFrame\\ncolumn_to_move = df[column_name]\\ncol = df.drop(column_name, axis=1, inplace=True)\\n\\n# Append the column back to the end of the DataFrame\\ndf[column_name] = column_to_move\\n\\n# select only rows where x_e out DOES exist\\ndata_test = df[df[\"x_e_out [-]\"].isna()]\\n\\n# start with a small data set for speed\\ndata_test = data_test[0:10]\\ndata_test = data_test.reset_index(drop=True)\\n\\n# Convert numerical values to string format to match BERT input requirement\\ndata_test = data_test.astype(str)\\n\\ndata_test[\"sequence\"] = \"\"\\n\\n# Concatenate all the values in a row into a single string using the column names\\n# Iterate through rows and columns\\nfor index, row in data_test.iterrows():\\n    string = \"\"\\n    for column in data_test.columns:\\n        if column == \"x_e_out [-]\": \\n            # make a mask with 4 sequential tokens\\n            string += column + \": \" + \\'[MASK]\\'*4 + \" \"\\n            continue\\n        if column == \"sequence\" or column == \\'id\\':\\n            continue\\n        string += column + \": \" + str(row[column]) + \" \"\\n    masked_string = string.strip()\\n    data_test[\"sequence\"][index] = masked_string\\n\\ndata_test.describe\\nprint(data_test[\"sequence\"][0]) '"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" # Load the numerical data you want to train BERT on\n",
    "df1 = pd.read_csv(\"data.csv\")\n",
    "#df2 = pd.read_csv('Data_CHF_Zhao_2020_ATE.csv')\n",
    "frames = [df1]\n",
    "df = pd.concat(frames)\n",
    "\n",
    "# Define the name of the column that you want to move to the end of the DataFrame\n",
    "column_name = \"x_e_out [-]\"\n",
    "\n",
    "# Select the column and drop it from the DataFrame\n",
    "column_to_move = df[column_name]\n",
    "col = df.drop(column_name, axis=1, inplace=True)\n",
    "\n",
    "# Append the column back to the end of the DataFrame\n",
    "df[column_name] = column_to_move\n",
    "\n",
    "# select only rows where x_e out DOES exist\n",
    "data_test = df[df[\"x_e_out [-]\"].isna()]\n",
    "\n",
    "# start with a small data set for speed\n",
    "data_test = data_test[0:10]\n",
    "data_test = data_test.reset_index(drop=True)\n",
    "\n",
    "# Convert numerical values to string format to match BERT input requirement\n",
    "data_test = data_test.astype(str)\n",
    "\n",
    "data_test[\"sequence\"] = \"\"\n",
    "\n",
    "# Concatenate all the values in a row into a single string using the column names\n",
    "# Iterate through rows and columns\n",
    "for index, row in data_test.iterrows():\n",
    "    string = \"\"\n",
    "    for column in data_test.columns:\n",
    "        if column == \"x_e_out [-]\": \n",
    "            # make a mask with 4 sequential tokens\n",
    "            string += column + \": \" + '[MASK]'*4 + \" \"\n",
    "            continue\n",
    "        if column == \"sequence\" or column == 'id':\n",
    "            continue\n",
    "        string += column + \": \" + str(row[column]) + \" \"\n",
    "    masked_string = string.strip()\n",
    "    data_test[\"sequence\"][index] = masked_string\n",
    "\n",
    "data_test.describe\n",
    "print(data_test[\"sequence\"][0]) \"\"\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading All Training/Testing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.describe of          id        author geometry pressure [MPa] mass_flux [kg/m2-s]  \\\n",
       "0         0      Thompson     tube            7.0              3770.0   \n",
       "1         1      Thompson     tube            nan              6049.0   \n",
       "2         2      Thompson      nan          13.79              2034.0   \n",
       "3         3          Beus  annulus          13.79              3679.0   \n",
       "4         5           nan      nan          17.24              3648.0   \n",
       "...     ...           ...      ...            ...                 ...   \n",
       "23089  1861  Richenderfer    plate           1.01              1500.0   \n",
       "23090  1862  Richenderfer    plate           1.01              1500.0   \n",
       "23091  1863  Richenderfer    plate           1.01              2000.0   \n",
       "23092  1864  Richenderfer    plate           1.01              2000.0   \n",
       "23093  1865  Richenderfer    plate           1.01              2000.0   \n",
       "\n",
       "      D_e [mm] D_h [mm] length [mm] chf_exp [MW/m2] x_e_out [-]  \\\n",
       "0          nan     10.8       432.0             3.6      0.1754   \n",
       "1         10.3     10.3       762.0             6.2     -0.0416   \n",
       "2          7.7      7.7       457.0             2.5      0.0335   \n",
       "3          5.6     15.2      2134.0             3.0     -0.0279   \n",
       "4          nan      1.9       696.0             3.6     -0.0711   \n",
       "...        ...      ...         ...             ...         ...   \n",
       "23089     15.0    120.0        10.0             9.4     -0.0218   \n",
       "23090     15.0    120.0        10.0            10.4     -0.0434   \n",
       "23091     15.0    120.0        10.0            10.8     -0.0109   \n",
       "23092     15.0    120.0        10.0            10.9     -0.0218   \n",
       "23093     15.0    120.0        10.0            11.5     -0.0434   \n",
       "\n",
       "                                                sequence  \n",
       "0      author: Thompson geometry: tube pressure [MPa]...  \n",
       "1      author: Thompson geometry: tube pressure [MPa]...  \n",
       "2      author: Thompson geometry: nan pressure [MPa]:...  \n",
       "3      author: Beus geometry: annulus pressure [MPa]:...  \n",
       "4      author: nan geometry: nan pressure [MPa]: 17.2...  \n",
       "...                                                  ...  \n",
       "23089  author: Richenderfer geometry: plate pressure ...  \n",
       "23090  author: Richenderfer geometry: plate pressure ...  \n",
       "23091  author: Richenderfer geometry: plate pressure ...  \n",
       "23092  author: Richenderfer geometry: plate pressure ...  \n",
       "23093  author: Richenderfer geometry: plate pressure ...  \n",
       "\n",
       "[23094 rows x 11 columns]>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the numerical data you want to train BERT on\n",
    "df1 = pd.read_csv(\"data.csv\")\n",
    "df2 = pd.read_csv('Data_CHF_Zhao_2020_ATE.csv')\n",
    "frames = [df1, df2]\n",
    "df = pd.concat(frames)\n",
    "\n",
    "# Define the name of the column that you want to move to the end of the DataFrame\n",
    "column_name = \"x_e_out [-]\"\n",
    "\n",
    "# Select the column and drop it from the DataFrame\n",
    "column_to_move = df[column_name]\n",
    "col = df.drop(column_name, axis=1, inplace=True)\n",
    "\n",
    "# Append the column back to the end of the DataFrame\n",
    "df[column_name] = column_to_move\n",
    "\n",
    "# select only rows where x_e out exists\n",
    "#data = df[df[\"x_e_out [-]\"].isna()]\n",
    "data = df[~df[\"x_e_out [-]\"].isna()]\n",
    "\n",
    "# start with a small data set for speed\n",
    "#data = data[0:1000]\n",
    "data = data.reset_index(drop=True)\n",
    "\n",
    "# Convert numerical values to string format to match BERT input requirement\n",
    "data = data.astype(str)\n",
    "\n",
    "data[\"sequence\"] = \"\"\n",
    "\n",
    "# Concatenate all the values in a row into a single string using the column names\n",
    "# Iterate through rows and columns\n",
    "for index, row in data.iterrows():\n",
    "    string = \"\"\n",
    "    for column in data.columns:\n",
    "        if column == \"x_e_out [-]\": \n",
    "            # Do not add mask tokens, simply ignore\n",
    "            #string += column + \": \" + '[MASK]'*4 + \" \"\n",
    "            continue\n",
    "        if column == \"sequence\" or column == 'id':\n",
    "            continue\n",
    "        string += column + \": \" + str(row[column]) + \" \"\n",
    "    masked_string = string.strip()\n",
    "    data[\"sequence\"][index] = masked_string\n",
    "\n",
    "data.describe"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train/Optimize \n",
    "BERT Masked LM in PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Make the tokenizer\n",
    "MAX_LENGTH = 128\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "token_encoder_fun = lambda row: tokenizer.encode(row,\n",
    "                                                 add_special_tokens=True,\n",
    "                                                 padding=\"max_length\",\n",
    "                                                 #return_tensors='pt',\n",
    "                                                 max_length=MAX_LENGTH,\n",
    "                                                 )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08d1937f8a21437cabcb54258433e793",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/23094 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Prepare data as Ax = B\n",
    "# A\n",
    "sequences = data[\"sequence\"]\n",
    "# B\n",
    "x_e_out = data['x_e_out [-]']\n",
    "\n",
    "tokenized_data = sequences.apply(token_encoder_fun)\n",
    "tokenized_data = tokenized_data.reset_index(drop=True)\n",
    "\n",
    "input_ids = torch.tensor(tokenized_data)\n",
    "\n",
    "attention_masks = torch.empty( ( len(input_ids), MAX_LENGTH ) )\n",
    "\n",
    "# Generate attention masks\n",
    "for i in trange(len(input_ids)):\n",
    "    tokens = input_ids[i, :]\n",
    "    row_mask = [int(token_id.item() > 0) for token_id in tokens]\n",
    "    row_mask = torch.tensor(row_mask).unsqueeze(0)\n",
    "    attention_masks[i] = row_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Size mismatch between tensors",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[49], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m generator \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mGenerator()\u001b[39m.\u001b[39mmanual_seed(\u001b[39m42\u001b[39m)\n\u001b[1;32m      6\u001b[0m \u001b[39m# Combine the training inputs into a TensorDataset.\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m dataset \u001b[39m=\u001b[39m TensorDataset(input_ids, attention_masks, x_e_out)\n\u001b[1;32m      9\u001b[0m \u001b[39m# Create a 90-10 train-validation split.\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \n\u001b[1;32m     11\u001b[0m \u001b[39m# Calculate the number of samples to include in each set.\u001b[39;00m\n\u001b[1;32m     12\u001b[0m train_size \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(\u001b[39m0.9\u001b[39m \u001b[39m*\u001b[39m \u001b[39mlen\u001b[39m(dataset))\n",
      "File \u001b[0;32m~/anaconda3/envs/STA208_BERT/lib/python3.9/site-packages/torch/utils/data/dataset.py:192\u001b[0m, in \u001b[0;36mTensorDataset.__init__\u001b[0;34m(self, *tensors)\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39mtensors: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 192\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39mall\u001b[39m(tensors[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39msize(\u001b[39m0\u001b[39m) \u001b[39m==\u001b[39m tensor\u001b[39m.\u001b[39msize(\u001b[39m0\u001b[39m) \u001b[39mfor\u001b[39;00m tensor \u001b[39min\u001b[39;00m tensors), \u001b[39m\"\u001b[39m\u001b[39mSize mismatch between tensors\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    193\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtensors \u001b[39m=\u001b[39m tensors\n",
      "\u001b[0;31mAssertionError\u001b[0m: Size mismatch between tensors"
     ]
    }
   ],
   "source": [
    "x_e_out = data['x_e_out [-]'].astype(float)\n",
    "x_e_out = torch.tensor(x_e_out, dtype=torch.float32).reshape(-1,1)\n",
    "\n",
    "generator = torch.Generator().manual_seed(42)\n",
    "\n",
    "# Combine the training inputs into a TensorDataset.\n",
    "dataset = TensorDataset(input_ids, attention_masks, x_e_out)\n",
    "\n",
    "# Create a 90-10 train-validation split.\n",
    "\n",
    "# Calculate the number of samples to include in each set.\n",
    "train_size = int(0.9 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "\n",
    "# Divide the dataset by randomly selecting samples.\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "            train_dataset,  # The training samples.\n",
    "            sampler = RandomSampler(train_dataset), # Select batches randomly\n",
    "            batch_size = batch_size # Trains with this batch size.\n",
    "        )\n",
    "\n",
    "# For validation the order doesn't matter, so we'll just read them sequentially.\n",
    "test_dataloader = DataLoader(\n",
    "            test_dataset, # The validation samples.\n",
    "            sampler = SequentialSampler(test_dataset), # Pull out batches sequentially.\n",
    "            batch_size = batch_size # Evaluate with this batch size.\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertHeatFlux(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BertHeatFlux, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained('bert-base-uncased').to(device)\n",
    "        self.dropout = torch.nn.Dropout(0.3)\n",
    "        # x[1].shape[-1]\n",
    "        self.linear = torch.nn.Linear(768, 1)\n",
    "    def forward(self, input_ids, attention_masks):\n",
    "        ouput = self.bert(input_ids, attention_masks)\n",
    "        pooled_output = output[1]\n",
    "        dropout = self.dropout(pooled_output)\n",
    "        out = self.linear(dropout)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = BertHeatFlux().to(device)\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61f224affbdc4843aa8b6152b1629b07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.002\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Define the optimizer and loss fn\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "\n",
    "# Step 4: Train the model\n",
    "num_epochs = 1\n",
    "\n",
    "for epoch in trange(num_epochs):\n",
    "    for batch in tqdm(train_dataloader):\n",
    "        input_ids, attention_mask, labels = [x.to(device) for x in batch]\n",
    "\n",
    "        # Reset gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        predictions = model(input_ids, \n",
    "                        attention_mask)\n",
    "        loss = loss_fn(predictions, labels)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        #torch.nn.utils.clip_grad\n",
    "\n",
    "        # Update the weights\n",
    "        optimizer.step()\n",
    "\n",
    "    # Print the loss and accuracy of the model for this epoch\n",
    "    print(f\"Epoch {epoch+1}, Loss: {loss:.3f}\")\n",
    "\n",
    "torch.save(model, 'bert_fine-tuned-7.sav')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "90/10 train/test split\n",
    "\n",
    "##### 7 Sequential `[MASK]` tokens\n",
    "- `bert_fine-tuned_1.sav`\n",
    "  - epochs=2, training: data.csv\n",
    "#\n",
    "- `bert_fine-tuned_2.sav`\n",
    "  - epochs=2, training: data.csv + Data_CHF\n",
    "#\n",
    "- `bert_fine-tuned_3.sav`\n",
    "  - epochs=10, training: data + Data_CHF\n",
    "#\n",
    "- `bert_fine-tuned_4.sav`\n",
    "  - epochs=5, training: Data_CHF\n",
    "#\n",
    "- `bert_fine-tuned_5.sav`\n",
    "  - epochs=10, training: Data_CHF\n",
    "\n",
    "##### 4 Sequential `[MASK]` tokens\n",
    "#\n",
    "- `bert_fine-tuned_6.sav`\n",
    "  - epochs=5, training: data + Data_CHF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e9c3d87c7e14e54b2e50fef4023a0e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/33 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "TypeError",
     "evalue": "mean() received an invalid combination of arguments - got (out=NoneType, dtype=NoneType, axis=NoneType, ), but expected one of:\n * (*, torch.dtype dtype)\n * (tuple of ints dim, bool keepdim, *, torch.dtype dtype)\n * (tuple of names dim, bool keepdim, *, torch.dtype dtype)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[79], line 13\u001b[0m\n\u001b[1;32m      9\u001b[0m     outputs \u001b[39m=\u001b[39m model(input_ids, \n\u001b[1;32m     10\u001b[0m                     attention_mask)\n\u001b[1;32m     12\u001b[0m     \u001b[39m# Calculate the accuracy of the model on this batch\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m     loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mmean(np\u001b[39m.\u001b[39;49msquare(outputs\u001b[39m.\u001b[39;49mcpu() \u001b[39m-\u001b[39;49m labels\u001b[39m.\u001b[39;49mcpu()))\n\u001b[1;32m     15\u001b[0m \u001b[39m# Print the accuracy of the model\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mMSE: \u001b[39m\u001b[39m{\u001b[39;00mnp\u001b[39m.\u001b[39msqrt(loss) \u001b[39m:\u001b[39;00m\u001b[39m.3f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36mmean\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/envs/STA208_BERT/lib/python3.9/site-packages/numpy/core/fromnumeric.py:3462\u001b[0m, in \u001b[0;36mmean\u001b[0;34m(a, axis, dtype, out, keepdims, where)\u001b[0m\n\u001b[1;32m   3460\u001b[0m         \u001b[39mpass\u001b[39;00m\n\u001b[1;32m   3461\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 3462\u001b[0m         \u001b[39mreturn\u001b[39;00m mean(axis\u001b[39m=\u001b[39;49maxis, dtype\u001b[39m=\u001b[39;49mdtype, out\u001b[39m=\u001b[39;49mout, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   3464\u001b[0m \u001b[39mreturn\u001b[39;00m _methods\u001b[39m.\u001b[39m_mean(a, axis\u001b[39m=\u001b[39maxis, dtype\u001b[39m=\u001b[39mdtype,\n\u001b[1;32m   3465\u001b[0m                       out\u001b[39m=\u001b[39mout, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "\u001b[0;31mTypeError\u001b[0m: mean() received an invalid combination of arguments - got (out=NoneType, dtype=NoneType, axis=NoneType, ), but expected one of:\n * (*, torch.dtype dtype)\n * (tuple of ints dim, bool keepdim, *, torch.dtype dtype)\n * (tuple of names dim, bool keepdim, *, torch.dtype dtype)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Evaluate the model on the testing dataset\n",
    "accuracy = 0\n",
    "with torch.no_grad():\n",
    "  for batch in tqdm(test_dataloader):\n",
    "    input_ids, attention_mask, labels = [x.to(device) for x in batch]\n",
    "\n",
    "    # Get the logits for the masked tokens\n",
    "    #outputs = model(input_ids, attention_mask=attention_mask)\n",
    "    outputs = model(input_ids, \n",
    "                    attention_mask)\n",
    "    \n",
    "    # Calculate the accuracy of the model on this batch\n",
    "    loss += np.mean(np.square(outputs.cpu() - labels.cpu()))\n",
    "\n",
    "# Print the accuracy of the model\n",
    "print(f\"MSE: {np.sqrt(loss/test_samples) :.3f}\")\n",
    "torch.save(model, 'bert_fine-tuned-7.sav')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predict Using Fine-Tuned BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function tqdm.__del__ at 0x29fb14ee0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/ask/anaconda3/envs/STA208_BERT/lib/python3.9/site-packages/tqdm/std.py\", line 1145, in __del__\n",
      "    self.close()\n",
      "  File \"/Users/ask/anaconda3/envs/STA208_BERT/lib/python3.9/site-packages/tqdm/notebook.py\", line 283, in close\n",
      "    self.disp(bar_style='danger', check_delay=False)\n",
      "AttributeError: 'tqdm_notebook' object has no attribute 'disp'\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "857af2af8dcd4d2987a77ee1e21aebb1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9525 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the numerical data you want to train BERT on\n",
    "df1 = pd.read_csv(\"data.csv\")\n",
    "#df2 = pd.read_csv('Data_CHF_Zhao_2020_ATE.csv')\n",
    "frames = [df1]\n",
    "df = pd.concat(frames)\n",
    "\n",
    "# Define the name of the column that you want to move to the end of the DataFrame\n",
    "column_name = \"x_e_out [-]\"\n",
    "\n",
    "# Select the column and drop it from the DataFrame\n",
    "column_to_move = df[column_name]\n",
    "col = df.drop(column_name, axis=1, inplace=True)\n",
    "\n",
    "# Append the column back to the end of the DataFrame\n",
    "df[column_name] = column_to_move\n",
    "\n",
    "# select only rows where x_e out DOES NOT exist\n",
    "data_test = df[df[\"x_e_out [-]\"].isna()]\n",
    "\n",
    "# start with a small data set for speed\n",
    "data_test = data_test[890:]\n",
    "data_test = data_test.reset_index(drop=True)\n",
    "\n",
    "# Convert numerical values to string format to match BERT input requirement\n",
    "data_test = data_test.astype(str)\n",
    "\n",
    "data_test[\"sequence\"] = \"\"\n",
    "\n",
    "# Concatenate all the values in a row into a single string using the column names\n",
    "# Iterate through rows and columns\n",
    "for index, row in data_test.iterrows():\n",
    "    string = \"\"\n",
    "    for column in data_test.columns:\n",
    "        if column == \"x_e_out [-]\": \n",
    "            # make a mask with 4 sequential tokens\n",
    "            string += column + \": \" + '[MASK]'*4 + \" \"\n",
    "            continue\n",
    "        if column == \"sequence\" or column == 'id':\n",
    "            continue\n",
    "        string += column + \": \" + str(row[column]) + \" \"\n",
    "    masked_string = string.strip()\n",
    "    data_test[\"sequence\"][index] = masked_string\n",
    "\n",
    "#data_test.describe\n",
    "\n",
    "\n",
    "sequences = data_test[\"sequence\"]\n",
    "x_e_out = data_test['x_e_out [-]']\n",
    "x_e_out = data_test['x_e_out [-]'].astype(float)\n",
    "x_e_out = torch.tensor(x_e_out, dtype=torch.float32).reshape(-1,1)\n",
    "\n",
    "tokenized_data = sequences.apply(token_encoder_fun)\n",
    "tokenized_data = tokenized_data.reset_index(drop=True)\n",
    "\n",
    "input_ids = torch.tensor(tokenized_data)\n",
    "#labels = torch.tensor(tokenized_labels)\n",
    "\n",
    "attention_masks = torch.empty( ( len(input_ids), MAX_LENGTH ) )\n",
    "\n",
    "# Generate attention masks\n",
    "for i in trange(len(input_ids)):\n",
    "    tokens = input_ids[i, :]\n",
    "    row_mask = [int(token_id.item() > 0) for token_id in tokens]\n",
    "    row_mask = torch.tensor(row_mask).unsqueeze(0)\n",
    "    attention_masks[i] = row_mask\n",
    "\n",
    "# For validation the order doesn't matter, so we'll just read them sequentially.\n",
    "eval_dataset = TensorDataset(input_ids, attention_masks, x_e_out)\n",
    "\n",
    "eval_dataloader = DataLoader(\n",
    "            dataset, # The validation samples.\n",
    "            sampler = SequentialSampler(test_dataset), # Pull out batches sequentially\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ecffd55ea8d4d9e82c407029c776feb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "178a8a5cb5ed43a490c3441e886793fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "IndexError",
     "evalue": "index 1 is out of bounds for dimension 0 with size 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[74], line 11\u001b[0m\n\u001b[1;32m      7\u001b[0m pred \u001b[39m=\u001b[39m model(input_ids, \n\u001b[1;32m      8\u001b[0m                 attention_mask)\n\u001b[1;32m     10\u001b[0m \u001b[39m# Print the loss and accuracy of the model for this epoch\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m predictions[i] \u001b[39m=\u001b[39m pred\n\u001b[1;32m     12\u001b[0m i \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 1 is out of bounds for dimension 0 with size 1"
     ]
    }
   ],
   "source": [
    "predictions = torch.empty( ( len(input_ids), 1 ) ).to(device)\n",
    "i = 0\n",
    "for batch in eval_dataloader:\n",
    "    input_ids, attention_mask, labels = [x.to(device) for x in tqdm(batch)]\n",
    "\n",
    "    # Forward pass\n",
    "    pred = model(input_ids, \n",
    "                    attention_mask)\n",
    "    \n",
    "    # Print the loss and accuracy of the model for this epoch\n",
    "    predictions[i] = pred\n",
    "    i += 1\n",
    "    #print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('batch2.txt', predictions.cpu().detach().numpy())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Appendix"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Old Snippets"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Console/Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([  101,  3166,  1024, 16660, 10988,  1024,  7270,  3778,  1031,  6131,\n",
       "          2050,  1033,  1024,  1020,  1012,  6535,  3742,  1035, 19251,  1031,\n",
       "          4705,  1013, 25525,  1011,  1055,  1033,  1024, 18432,  1012,  1014,\n",
       "          1040,  1035,  1041,  1031,  3461,  1033,  1024,  2184,  1012,  1022,\n",
       "          1040,  1035,  1044,  1031,  3461,  1033,  1024,  2184,  1012,  1022,\n",
       "          3091,  1031,  3461,  1033,  1024,  4724,  2475,  1012,  1014, 10381,\n",
       "          2546,  1035,  4654,  2361,  1031, 12464,  1013, 25525,  1033,  1024,\n",
       "          1018,  1012,  1018,  1060,  1035,  1041,  1035,  2041,  1031,  1011,\n",
       "          1033,  1024,   103,   103,   103,   103,   102,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0],\n",
       "        device='mps:0'),\n",
       " tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0.], device='mps:0'),\n",
       " tensor([0.0330]))"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0, device='mps:0')\n"
     ]
    }
   ],
   "source": [
    "i = torch.argmax(probs)\n",
    "print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [ t for t in input_ids[0] if all([t!=0, t !=101, t !=102]) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[ S E P ]'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(102)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<generator object Module.named_modules at 0x4351c05f0>\n"
     ]
    }
   ],
   "source": [
    "model = BertModel.from_pretrained('bert-base-uncased').to(device)\n",
    "print(model.named_modules())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

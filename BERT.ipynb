{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BERT - Bidirectional Encoder Representations from Transformers - is a state-of-the-art natural language processing model that has produced impressive results in a variety of text-based tasks. However, it is a pre-trained model and often requires fine-tuning on specific tasks to achieve optimal performance.\n",
    "\n",
    "To fine-tune a BERT model on a special set of numerical data with random values missing, we'll need to follow these steps:\n",
    "\n",
    "1. Prepare the Data: We first need to prepare our data in a usable format for BERT. This involves converting our numerical dataset into a textual format that BERT can understand. For example, we can convert our input data to a tab-separated sequence of values, where the missing values are represented as `[MASK]`.\n",
    "\n",
    "2. Load the Pre-Trained Model: Next, we load a pre-trained BERT model that corresponds to our specific problem and task. There are several pre-trained BERT models available in the Hugging Face transformer library, such as bert-base-uncased, bert-large-uncased, and others.\n",
    "\n",
    "3. Fine-Tune the Model: After preparing our data and loading the pre-trained model, we can fine-tune the model on our specific task. This involves training the model on our numerical data while updating the weights of the pre-trained BERT model based on the gradients of our dataset. We typically use a GPU to speed up the process of fine-tuning the model, as it is a computationally expensive task.\n",
    "\n",
    "4. Evaluate the Model: Once we have fine-tuned our model, we evaluate its performance on a separate testing dataset. This allows us to verify the accuracy of our fine-tuned model and determine its suitability for our problem.\n",
    "\n",
    "Step 1: Prepare the data\n",
    "- Convert the numerical data to a text format that BERT can understand.\n",
    "- Randomly remove values from the data and replace them with `[MASK]` tokens.\n",
    "- Split the dataset into training and testing sets.\n",
    "\n",
    "Step 2: Load the pre-trained model\n",
    "- Import the pre-trained BERT model from the Hugging Face transformer library.\n",
    "- Define the hyperparameters for the model, such as the learning rate, batch size, and number of epochs.\n",
    "- Instantiate the model and load the pre-trained weights.\n",
    "\n",
    "Step 3: Fine-tune the model\n",
    "- Using the training dataset, fine-tune the pre-trained BERT model and update its weights.\n",
    "- Use a GPU to speed up the training process, if possible.\n",
    "- Monitor the loss and accuracy of the model during training to determine when to stop.\n",
    "\n",
    "Step 4: Evaluate the model\n",
    "- Use the testing dataset to evaluate the accuracy of the fine-tuned BERT model.\n",
    "- Use metrics such as accuracy, precision, recall, and F1 score to measure the performance of the model."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/ask/anaconda3/envs/STA208_BERT/bin/python\n",
      "platform: macOS-13.4-arm64-arm-64bit\n",
      "python 3.9.16\n",
      "pandas 1.5.3\n",
      "numpy 1.24.3\n",
      "torch 2.0.1\n",
      "GPU is NOT available\n",
      "MPS is AVAILABLE\n",
      "target device is mps\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader, random_split, SequentialSampler\n",
    "\n",
    "#\n",
    "from tqdm.notebook import tqdm, trange\n",
    "\n",
    "from platform import python_version, platform\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# torch.set_num_threads(2)\n",
    "\n",
    "from transformers import (\n",
    "    BertModel,\n",
    "    BertTokenizer,\n",
    "    TFBertForMaskedLM,\n",
    "    BertForMaskedLM,\n",
    "    AdamW,\n",
    "    get_linear_schedule_with_warmup,\n",
    ")\n",
    " \n",
    "from sys import executable\n",
    "print(executable)\n",
    "print(\"platform: {}\".format(platform())) #Python Platform: macOS-13.3.1-arm64-arm-64bit\n",
    "print('python ' + python_version())\n",
    "print(pd.__name__, pd.__version__)\n",
    "print(np.__name__, np.__version__)\n",
    "print(torch.__name__, torch.__version__)\n",
    "has_gpu = torch.cuda.is_available()\n",
    "has_mps = getattr(torch,'has_mps',False)\n",
    "device = \"mps\" if has_mps \\\n",
    "    else \"gpu\" if has_gpu else \"cpu\"\n",
    "\n",
    "print(\"GPU is\", \"AVAILABLE\" if has_gpu else \"NOT available\")#GPU is NOT AVAILABLE\n",
    "print(\"MPS is\", \"AVAILABLE\" if has_mps else \"NOT available\") #MPS is AVAILABLE\n",
    " \n",
    "print(\"target device is {}\".format(device)) #Target device is mps"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Processing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Evaluation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "author: nan geometry: tube pressure [MPa]: 13.79 mass_flux [kg/m2-s]: 686.0 D_e [mm]: 11.1 D_h [mm]: 11.1 length [mm]: 457.0 chf_exp [MW/m2]: 2.8 x_e_out [-]: [MASK][MASK][MASK][MASK]\n"
     ]
    }
   ],
   "source": [
    "# Load the numerical data you want to train BERT on\n",
    "df1 = pd.read_csv(\"data.csv\")\n",
    "#df2 = pd.read_csv('Data_CHF_Zhao_2020_ATE.csv')\n",
    "frames = [df1]\n",
    "df = pd.concat(frames)\n",
    "\n",
    "# Define the name of the column that you want to move to the end of the DataFrame\n",
    "column_name = \"x_e_out [-]\"\n",
    "\n",
    "# Select the column and drop it from the DataFrame\n",
    "column_to_move = df[column_name]\n",
    "col = df.drop(column_name, axis=1, inplace=True)\n",
    "\n",
    "# Append the column back to the end of the DataFrame\n",
    "df[column_name] = column_to_move\n",
    "\n",
    "# select only rows where x_e out DOES exist\n",
    "data_test = df[df[\"x_e_out [-]\"].isna()]\n",
    "\n",
    "# start with a small data set for speed\n",
    "data_test = data_test[0:10]\n",
    "data_test = data_test.reset_index(drop=True)\n",
    "\n",
    "# Convert numerical values to string format to match BERT input requirement\n",
    "data_test = data_test.astype(str)\n",
    "\n",
    "data_test[\"sequence\"] = \"\"\n",
    "\n",
    "# Concatenate all the values in a row into a single string using the column names\n",
    "# Iterate through rows and columns\n",
    "for index, row in data_test.iterrows():\n",
    "    string = \"\"\n",
    "    for column in data_test.columns:\n",
    "        if column == \"x_e_out [-]\": \n",
    "            # make a mask with 4 sequential tokens\n",
    "            string += column + \": \" + '[MASK]'*4 + \" \"\n",
    "            continue\n",
    "        if column == \"sequence\" or column == 'id':\n",
    "            continue\n",
    "        string += column + \": \" + str(row[column]) + \" \"\n",
    "    masked_string = string.strip()\n",
    "    data_test[\"sequence\"][index] = masked_string\n",
    "\n",
    "data_test.describe\n",
    "print(data_test[\"sequence\"][0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading All Training/Testing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.describe of          id        author geometry pressure [MPa] mass_flux [kg/m2-s]  \\\n",
       "0         0      Thompson     tube            7.0              3770.0   \n",
       "1         1      Thompson     tube            nan              6049.0   \n",
       "2         2      Thompson      nan          13.79              2034.0   \n",
       "3         3          Beus  annulus          13.79              3679.0   \n",
       "4         5           nan      nan          17.24              3648.0   \n",
       "...     ...           ...      ...            ...                 ...   \n",
       "23089  1861  Richenderfer    plate           1.01              1500.0   \n",
       "23090  1862  Richenderfer    plate           1.01              1500.0   \n",
       "23091  1863  Richenderfer    plate           1.01              2000.0   \n",
       "23092  1864  Richenderfer    plate           1.01              2000.0   \n",
       "23093  1865  Richenderfer    plate           1.01              2000.0   \n",
       "\n",
       "      D_e [mm] D_h [mm] length [mm] chf_exp [MW/m2] x_e_out [-]  \\\n",
       "0          nan     10.8       432.0             3.6      0.1754   \n",
       "1         10.3     10.3       762.0             6.2     -0.0416   \n",
       "2          7.7      7.7       457.0             2.5      0.0335   \n",
       "3          5.6     15.2      2134.0             3.0     -0.0279   \n",
       "4          nan      1.9       696.0             3.6     -0.0711   \n",
       "...        ...      ...         ...             ...         ...   \n",
       "23089     15.0    120.0        10.0             9.4     -0.0218   \n",
       "23090     15.0    120.0        10.0            10.4     -0.0434   \n",
       "23091     15.0    120.0        10.0            10.8     -0.0109   \n",
       "23092     15.0    120.0        10.0            10.9     -0.0218   \n",
       "23093     15.0    120.0        10.0            11.5     -0.0434   \n",
       "\n",
       "                                                sequence  \n",
       "0      author: Thompson geometry: tube pressure [MPa]...  \n",
       "1      author: Thompson geometry: tube pressure [MPa]...  \n",
       "2      author: Thompson geometry: nan pressure [MPa]:...  \n",
       "3      author: Beus geometry: annulus pressure [MPa]:...  \n",
       "4      author: nan geometry: nan pressure [MPa]: 17.2...  \n",
       "...                                                  ...  \n",
       "23089  author: Richenderfer geometry: plate pressure ...  \n",
       "23090  author: Richenderfer geometry: plate pressure ...  \n",
       "23091  author: Richenderfer geometry: plate pressure ...  \n",
       "23092  author: Richenderfer geometry: plate pressure ...  \n",
       "23093  author: Richenderfer geometry: plate pressure ...  \n",
       "\n",
       "[23094 rows x 11 columns]>"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the numerical data you want to train BERT on\n",
    "df1 = pd.read_csv(\"data.csv\")\n",
    "df2 = pd.read_csv('Data_CHF_Zhao_2020_ATE.csv')\n",
    "frames = [df1, df2]\n",
    "df = pd.concat(frames)\n",
    "\n",
    "# Define the name of the column that you want to move to the end of the DataFrame\n",
    "column_name = \"x_e_out [-]\"\n",
    "\n",
    "# Select the column and drop it from the DataFrame\n",
    "column_to_move = df[column_name]\n",
    "col = df.drop(column_name, axis=1, inplace=True)\n",
    "\n",
    "# Append the column back to the end of the DataFrame\n",
    "df[column_name] = column_to_move\n",
    "\n",
    "# select only rows where x_e out exists\n",
    "data = df[~df[\"x_e_out [-]\"].isna()]\n",
    "\n",
    "# start with a small data set for speed\n",
    "#data = data[0:100]\n",
    "data = data.reset_index(drop=True)\n",
    "\n",
    "# Convert numerical values to string format to match BERT input requirement\n",
    "data = data.astype(str)\n",
    "\n",
    "data[\"sequence\"] = \"\"\n",
    "\n",
    "# Concatenate all the values in a row into a single string using the column names\n",
    "# Iterate through rows and columns\n",
    "for index, row in data.iterrows():\n",
    "    string = \"\"\n",
    "    for column in data.columns:\n",
    "        if column == \"x_e_out [-]\": \n",
    "            # make a mask with 7 sequential tokens\n",
    "            string += column + \": \" + '[MASK]'*4 + \" \"\n",
    "            continue\n",
    "        if column == \"sequence\" or column == 'id':\n",
    "            continue\n",
    "        string += column + \": \" + str(row[column]) + \" \"\n",
    "    masked_string = string.strip()\n",
    "    data[\"sequence\"][index] = masked_string\n",
    "\n",
    "data.describe"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train/Optimize \n",
    "BERT Masked LM in PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Make the tokenizer\n",
    "MAX_LENGTH = 128\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "token_encoder_fun = lambda row: tokenizer.encode(row,\n",
    "                                                 add_special_tokens=True,\n",
    "                                                 padding=\"max_length\",\n",
    "                                                 #return_tensors='pt',\n",
    "                                                 max_length=MAX_LENGTH,\n",
    "                                                 )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.115\n",
      "Epoch 2, Loss: 0.111\n",
      "Epoch 3, Loss: 0.097\n",
      "Epoch 4, Loss: 0.091\n",
      "Epoch 5, Loss: 0.075\n",
      "Accuracy: 125.077\n"
     ]
    }
   ],
   "source": [
    "model = BertForMaskedLM.from_pretrained(\"bert-base-uncased\",\n",
    "                                        return_dict=True,\n",
    "                                        pad_token_id = 0\n",
    "                                        ).to(device)\n",
    "\n",
    "model.train()\n",
    "\n",
    "sequences = data[\"sequence\"]\n",
    "x_e_out = data['x_e_out [-]']\n",
    "\n",
    "tokenized_data = sequences.apply(token_encoder_fun)\n",
    "tokenized_data = tokenized_data.reset_index(drop=True)\n",
    "\n",
    "tokenized_labels = x_e_out.apply(token_encoder_fun)\n",
    "tokenized_labels = tokenized_labels.reset_index(drop=True)\n",
    "\n",
    "input_ids = torch.tensor(tokenized_data)\n",
    "labels = torch.tensor(tokenized_labels)\n",
    "\n",
    "attention_masks = torch.empty( ( len(input_ids), MAX_LENGTH ) )\n",
    "\n",
    "# Generate attention masks\n",
    "for i in range(len(input_ids)):\n",
    "    tokens = input_ids[i, :]\n",
    "    row_mask = [int(token_id.item() > 0) for token_id in tokens]\n",
    "    row_mask = torch.tensor(row_mask).unsqueeze(0)\n",
    "    attention_masks[i] = row_mask\n",
    "    \n",
    "generator = torch.Generator().manual_seed(42)\n",
    "\n",
    "# Combine the training inputs into a TensorDataset.\n",
    "dataset = TensorDataset(input_ids, attention_masks, labels)\n",
    "\n",
    "# Create a 90-10 train-validation split.\n",
    "\n",
    "# Calculate the number of samples to include in each set.\n",
    "train_size = int(0.9 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "\n",
    "# Divide the dataset by randomly selecting samples.\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "# The DataLoader needs to know our batch size for training. \n",
    "# For fine-tuning BERT on a specific task, the authors recommend a batch \n",
    "# size of 16 or 32.\n",
    "batch_size = 32\n",
    "\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "# Create the DataLoaders for our training and validation sets.\n",
    "# We'll take training samples in random order. \n",
    "train_dataloader = DataLoader(\n",
    "            train_dataset,  # The training samples.\n",
    "            sampler = RandomSampler(train_dataset), # Select batches randomly\n",
    "            batch_size = batch_size # Trains with this batch size.\n",
    "        )\n",
    "\n",
    "# For validation the order doesn't matter, so we'll just read them sequentially.\n",
    "test_dataloader = DataLoader(\n",
    "            test_dataset, # The validation samples.\n",
    "            sampler = SequentialSampler(test_dataset), # Pull out batches sequentially.\n",
    "            batch_size = batch_size # Evaluate with this batch size.\n",
    "        )\n",
    "# Step 2: Define the masked language modeling (MLM) loss function\n",
    "mask_token_id = tokenizer.mask_token_id\n",
    "\n",
    "# Step 3: Define the optimizer and loss fn\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Step 4: Train the model\n",
    "num_epochs = 5\n",
    "\n",
    "for epoch in trange(num_epochs):\n",
    "    for batch in train_dataloader:\n",
    "        input_ids, attention_mask, labels = [x.to(device) for x in batch]\n",
    "\n",
    "        # Reset gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(input_ids, \n",
    "                        attention_mask=attention_mask, \n",
    "                        labels=labels)\n",
    "        loss = outputs.loss\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # Update the weights\n",
    "        optimizer.step()\n",
    "\n",
    "    # Print the loss and accuracy of the model for this epoch\n",
    "    print(f\"Epoch {epoch+1}, Loss: {loss:.3f}\")\n",
    "\n",
    "# Set model to evaluate mode\n",
    "model.eval()\n",
    "\n",
    "# Evaluate the model on the testing dataset\n",
    "accuracy = 0\n",
    "with torch.no_grad():\n",
    "  for batch in tqdm(test_dataloader):\n",
    "    input_ids, attention_mask, labels = [x.to(device) for x in batch]\n",
    "\n",
    "    # Get the logits for the masked tokens\n",
    "    #outputs = model(input_ids, attention_mask=attention_mask)\n",
    "    outputs = model(input_ids, \n",
    "                    attention_mask=attention_mask, \n",
    "                    labels=labels)\n",
    "    logits = outputs.logits\n",
    "\n",
    "    # Get the predictions for the masked tokens\n",
    "    _, predictions = torch.max(logits, dim=2)\n",
    "\n",
    "    # Calculate the accuracy of the model on this batch\n",
    "    accuracy += np.sum(predictions.cpu().numpy() == labels.cpu().numpy())\n",
    "\n",
    "# Print the accuracy of the model\n",
    "print(f\"Accuracy: {accuracy / test_size:.3f}\")\n",
    "torch.save(model, 'bert_fine-tuned_6.sav')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predict Using Fine-Tuned BERT"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "90/10 train/test split\n",
    "\n",
    "##### 7 Sequential `[MASK]` tokens\n",
    "- `bert_fine-tuned_1.sav`\n",
    "  - epochs=2, training: data.csv\n",
    "#\n",
    "- `bert_fine-tuned_2.sav`\n",
    "  - epochs=2, training: data.csv + Data_CHF\n",
    "#\n",
    "- `bert_fine-tuned_3.sav`\n",
    "  - epochs=10, training: data + Data_CHF\n",
    "#\n",
    "- `bert_fine-tuned_4.sav`\n",
    "  - epochs=5, training: Data_CHF\n",
    "#\n",
    "- `bert_fine-tuned_5.sav`\n",
    "  - epochs=10, training: Data_CHF\n",
    "\n",
    "##### 4 Sequential `[MASK]` tokens\n",
    "#\n",
    "- `bert_fine-tuned_6.sav`\n",
    "  - epochs=5, training: data + Data_CHF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28b5908d804745c4948a166378145b35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "IndexError",
     "evalue": "Dimension out of range (expected to be in range of [-1, 0], but got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[78], line 61\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[39m# convert the predicted token IDs back into tokens using the tokenizer\u001b[39;00m\n\u001b[1;32m     60\u001b[0m     decoded_tokens \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mclone(input_ids)\n\u001b[0;32m---> 61\u001b[0m     \u001b[39mfor\u001b[39;00m i, token_index \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(torch\u001b[39m.\u001b[39;49margmax(probs, dim\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)):\n\u001b[1;32m     62\u001b[0m         decoded_tokens[\u001b[39m0\u001b[39m][mask_index] \u001b[39m=\u001b[39m token_index\u001b[39m.\u001b[39mitem()\n\u001b[1;32m     64\u001b[0m trim_toks \u001b[39m=\u001b[39m [ t \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m decoded_tokens[\u001b[39m0\u001b[39m] \u001b[39mif\u001b[39;00m \u001b[39mall\u001b[39m([t\u001b[39m!=\u001b[39m\u001b[39m0\u001b[39m, t\u001b[39m!=\u001b[39m\u001b[39m101\u001b[39m, t\u001b[39m!=\u001b[39m\u001b[39m102\u001b[39m]) ]\n",
      "\u001b[0;31mIndexError\u001b[0m: Dimension out of range (expected to be in range of [-1, 0], but got 1)"
     ]
    }
   ],
   "source": [
    "mask_token_id = tokenizer.mask_token_id\n",
    "\n",
    "model_name = 'bert_fine-tuned_6.sav'\n",
    "model = torch.load(model_name).to('mps')\n",
    "\n",
    "sequences = data_test[\"sequence\"]\n",
    "x_e_out = data_test['x_e_out [-]']\n",
    "\n",
    "tokenized_data = sequences.apply(token_encoder_fun)\n",
    "tokenized_data = tokenized_data.reset_index(drop=True)\n",
    "\n",
    "input_ids = torch.tensor(tokenized_data).to(device)\n",
    "\n",
    "attention_masks = torch.empty( ( len(input_ids), MAX_LENGTH ) ).to(device)\n",
    "\n",
    "# Different attention mask to training since I only care about the [MASK] tokens for prediction\n",
    "for i in range(len(input_ids)):\n",
    "    tokens = input_ids[i, :]\n",
    "    row_mask = [int(token_id.item() == mask_token_id ) for token_id in tokens]\n",
    "    row_mask = torch.tensor(row_mask).unsqueeze(0)\n",
    "    attention_masks[i] = row_mask\n",
    "\n",
    "# Combine the training inputs into a TensorDataset.\n",
    "dataset = TensorDataset(input_ids, attention_masks)\n",
    "\n",
    "# For validation the order doesn't matter, so we'll just read them sequentially.\n",
    "prediction_dataloader = DataLoader(\n",
    "            dataset, # The validation samples.\n",
    "            sampler = SequentialSampler(dataset), # Pull out batches sequentially.\n",
    "        )\n",
    "\n",
    "# Empty list of decoded\n",
    "seqs_out = []\n",
    "\n",
    "for batch in tqdm(prediction_dataloader):\n",
    "    input_ids, attention_masks = [x.to(device) for x in batch]\n",
    "    # Get the logits for the masked tokens\n",
    "    #outputs = model(input_ids, attention_mask=attention_mask)\n",
    "    token_logits = model(input_ids, \n",
    "                    attention_mask=attention_masks).logits\n",
    "\n",
    "    # Find the location of [MASK] and extract its logits\n",
    "    #mask_token_index = np.argwhere(input_ids.cpu().numpy() == mask_token_id)\n",
    "    # locate all masked tokens and create a mask tensor\n",
    "    \n",
    "    mask_indices = [i for i, x in enumerate(input_ids[0]) if x.item() == tokenizer.mask_token_id]\n",
    "    #mask = torch.zeros(input_ids.shape, dtype=torch.long)\n",
    "    #mask[0, mask_indices] = 1\n",
    "    \n",
    "    # loop over mask indices and ID, replace tokens\n",
    "    for mask_index, attention_mask in zip(mask_indices, attention_masks):\n",
    "        # run the inputs through the model and extract the probabilities for each masked token\n",
    "        attention_mask = attention_mask.view(1, 128)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "            logits = outputs[0][0, mask_index, :]\n",
    "            probs = torch.softmax(logits, dim=-1).to(device)\n",
    "\n",
    "        # convert the predicted token IDs back into tokens using the tokenizer\n",
    "        decoded_tokens = torch.clone(input_ids)\n",
    "        for i, token_index in enumerate(torch.argmax(probs, dim=1)):\n",
    "            decoded_tokens[0][mask_index] = token_index.item()\n",
    "        \n",
    "    trim_toks = [ t for t in decoded_tokens[0] if all([t!=0, t!=101, t!=102]) ]\n",
    "\n",
    "    decoded_sequence = tokenizer.decode(trim_toks)\n",
    "    \n",
    "    #mask_token_logits = token_logits[0, tok_idx, :]\n",
    "\n",
    "\n",
    "        # apply softmax to normalize the logits into probabilities\n",
    "       # probs = torch.softmax(logits, dim=-1)\n",
    "\n",
    "        # get the index of the highest-probability token for each masked position\n",
    "       \n",
    "        #_, preds = torch.max(probs, dim=-1)\n",
    "        # replace the masked tokens in the input sequence with their predicted tokens\n",
    "        #input_ids.masked_fill_(mask_indices, preds)\n",
    "    \n",
    "        #top_tokens = np.argsort(-mask_token_logits.cpu().detach())[:].tolist()\n",
    "        # Remove special tokens that don't contain human-relevant information ([CLS] and [SEP])\n",
    "        #top_tokens = [tok for tok in top_tokens if all([tok!=0, tok !=101, tok !=102])]\n",
    "        \n",
    "        # Choose the most likely remaining token and replace masked token\n",
    "        #input_ids[0][tok_idx[1]] = top_tokens[0][0]\n",
    "\n",
    "    # Trim PAD, CLS, SEP (t !=102) tokens\n",
    "    #trim_toks = [ t for t in decoded_tokens[0] if all([t!=0, t!=101, t!=102]) ]\n",
    "    #s = tokenizer.decode(trim_toks)\n",
    "    print(decoded_sequence[-25:])\n",
    "    seqs_out.append(decoded_sequence)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Appendix"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Old Snippets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "101\n",
      "3166\n",
      "1024\n",
      "16660\n",
      "10988\n",
      "1024\n",
      "7270\n",
      "3778\n",
      "1031\n",
      "6131\n",
      "2050\n",
      "1033\n",
      "1024\n",
      "2410\n",
      "1012\n",
      "6535\n",
      "3742\n",
      "1035\n",
      "19251\n",
      "1031\n",
      "4705\n",
      "1013\n",
      "25525\n",
      "1011\n",
      "1055\n",
      "1033\n",
      "1024\n",
      "6273\n",
      "2575\n",
      "1012\n",
      "1014\n",
      "1040\n",
      "1035\n",
      "1041\n",
      "1031\n",
      "3461\n",
      "1033\n",
      "1024\n",
      "2340\n",
      "1012\n",
      "1015\n",
      "1040\n",
      "1035\n",
      "1044\n",
      "1031\n",
      "3461\n",
      "1033\n",
      "1024\n",
      "2340\n",
      "1012\n",
      "1015\n",
      "3091\n",
      "1031\n",
      "3461\n",
      "1033\n",
      "1024\n",
      "3429\n",
      "2581\n",
      "1012\n",
      "1014\n",
      "10381\n",
      "2546\n",
      "1035\n",
      "4654\n",
      "2361\n",
      "1031\n",
      "12464\n",
      "1013\n",
      "25525\n",
      "1033\n",
      "1024\n",
      "1016\n",
      "1012\n",
      "1022\n",
      "1060\n",
      "1035\n",
      "1041\n",
      "1035\n",
      "2041\n",
      "1031\n",
      "1011\n",
      "1033\n",
      "1024\n",
      "103\n",
      "103\n",
      "103\n",
      "103\n",
      "102\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "#torch.save(model, 'path/to/model')\n",
    "\n",
    "#saved_model = torch.load('path/to/model')\n",
    "#[i for i, x in enumerate(input_ids[0]) if x == tokenizer.mask_token_id]\n",
    "for i, x in enumerate(input_ids[0]):\n",
    "    print(x.item())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Console/Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ P A D ]\n",
      "# # 8\n",
      "2 5 9\n",
      "# # 3 3\n",
      "# # 7\n"
     ]
    }
   ],
   "source": [
    "for tok in top_tokens:\n",
    "    if tok != 101 and tok != 102:\n",
    "        print(tokenizer.decode(tok)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 128])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_mask.view(1, 128).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42f8baf63e1b4c4687b87161eb398d42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Outer Level:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e93d8168cc4447fc827604019863ef1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inner Level:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "NameError",
     "evalue": "name 'time' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m tqdm(outer_level, desc\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mOuter Level\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[1;32m      5\u001b[0m     \u001b[39mfor\u001b[39;00m number \u001b[39min\u001b[39;00m tqdm(inner_level, desc\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mInner Level\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[0;32m----> 6\u001b[0m         time\u001b[39m.\u001b[39msleep(\u001b[39m0.01\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'time' is not defined"
     ]
    }
   ],
   "source": [
    "outer_level = list(range(2))\n",
    "inner_level = list(range(100))\n",
    "for _ in tqdm(outer_level, desc='Outer Level'):\n",
    "    for number in tqdm(inner_level, desc='Inner Level'):\n",
    "        time.sleep(0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [ t for t in input_ids[0] if all([t!=0, t !=101, t !=102]) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[ S E P ]'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(102)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "101"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_tokens[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

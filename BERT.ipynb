{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import faulthandler\n",
    "\n",
    "faulthandler.enable()\n",
    "\n",
    "# Importing the necessary libraries\n",
    "import torch\n",
    "\n",
    "# torch.set_num_threads(2)\n",
    "\n",
    "import tensorflow as tf\n",
    "from transformers import (\n",
    "    BertModel,\n",
    "    BertTokenizer,\n",
    "    TFBertForMaskedLM,\n",
    "    BertForMaskedLM,\n",
    "    AdamW,\n",
    "    get_linear_schedule_with_warmup,\n",
    ")\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the numerical data you want to train BERT on\n",
    "df = pd.read_csv(\"data.csv\")\n",
    "\n",
    "# Select only rows where x_e_out not missing\n",
    "\n",
    "# Define the name of the column that you want to move to the end of the DataFrame\n",
    "column_name = \"x_e_out [-]\"\n",
    "\n",
    "# Select the column and drop it from the DataFrame\n",
    "column_to_move = df[column_name]\n",
    "col = df.drop(column_name, axis=1, inplace=True)\n",
    "\n",
    "# Append the column back to the end of the DataFrame\n",
    "df[column_name] = column_to_move\n",
    "\n",
    "# select only rows where x_e out exists\n",
    "data = df[~df[\"x_e_out [-]\"].isna()]\n",
    "\n",
    "# start with a small data set for speed\n",
    "data = data[0:100]\n",
    "data = data.reset_index(drop=True)\n",
    "\n",
    "# Convert numerical values to string format to match BERT input requirement\n",
    "data = data.astype(str)\n",
    "\n",
    "data[\"sequence\"] = \"\"\n",
    "\n",
    "# Concatenate all the values in a row into a single string using the column names\n",
    "# Iterate through rows and columns\n",
    "for index, row in data.iterrows():\n",
    "    string = \"\"\n",
    "    for column in data.columns:\n",
    "        if column != \"sequence\":\n",
    "            # Concatenate column name with row value\n",
    "            string += column + \": \" + str(row[column]) + \" \"\n",
    "    data[\"sequence\"][index] = string\n",
    "\n",
    "data.describe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertForMaskedLM\n",
    "from torch.nn import functional as F\n",
    "import torch\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = BertForMaskedLM.from_pretrained(\"bert-base-uncased\", return_dict=True)\n",
    "text = \"where are my \" + tokenizer.mask_token + \"?\"\n",
    "input = tokenizer.encode_plus(text, return_tensors=\"pt\")\n",
    "mask_index = torch.where(input[\"input_ids\"][0] == tokenizer.mask_token_id)\n",
    "output = model(**input)\n",
    "logits = output.logits\n",
    "softmax = F.softmax(logits, dim=-1)\n",
    "mask_word = softmax[0, mask_index, :]\n",
    "top_10 = torch.topk(mask_word, 10, dim=1)[1][0]\n",
    "for token in top_10:\n",
    "    word = tokenizer.decode([token])\n",
    "    new_sentence = text.replace(tokenizer.mask_token, word)\n",
    "    print(new_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test = df[df[\"x_e_out [-]\"].isna()]\n",
    "\n",
    "# start with a small data set for speed\n",
    "data_test = data_test[0:100]\n",
    "data_test = data_test.reset_index(drop=True)\n",
    "\n",
    "# Convert numerical values to string format to match BERT input requirement\n",
    "data_test = data_test.astype(str)\n",
    "\n",
    "data_test[\"sequence\"] = \"\"\n",
    "\n",
    "# Concatenate all the values in a row into a single string using the column names\n",
    "# Iterate through rows and columns\n",
    "for index, row in data_test.iterrows():\n",
    "    string = \"\"\n",
    "    for column in data_test.columns:\n",
    "        if column != \"sequence\":\n",
    "            # Concatenate column name with row value\n",
    "            string += column + \": \" + str(row[column]) + \" \"\n",
    "    data_test[\"sequence\"][index] = string\n",
    "\n",
    "data_test.describe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertForMaskedLM\n",
    "\n",
    "\n",
    "# Define the BERT Classifier\n",
    "class BERTSequenceImputer(torch.nn.Module):\n",
    "    def __init__(self, freeze_BERT=True, dropout_rate=0.3):\n",
    "        super(BERTSequenceImputer, self).__init__()\n",
    "\n",
    "        # Load the BERT model and tokenizer\n",
    "        # self.bert = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "        self.bert = BertForMaskedLM.from_pretrained(\n",
    "            \"bert-base-uncased\", return_dict=False\n",
    "        )\n",
    "\n",
    "        # Freeze the weights of the BERT model\n",
    "        if freeze_BERT:\n",
    "            for param in self.bert.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "        # Define the linear layer for classification\n",
    "        self.dropout = torch.nn.Dropout(dropout_rate)\n",
    "        self.fc = torch.nn.Linear(768, 1)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        # Use the BERT model to transform the sequence into an embedded format\n",
    "        embedded_seq = self.bert(\n",
    "            input_ids=input_ids, attention_mask=attention_mask, return_dict=False\n",
    "        )\n",
    "        embedded_seq = self.dropout(embedded_seq[0])\n",
    "\n",
    "        # Predict the missing value (in our case the last token) using the linear layer\n",
    "        output = self.fc(embedded_seq[:, -1])\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pretrained BERT model and tokenizer to convert data to tokenize-able format\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "# model = BertForMaskedLM.from_pretrained(\"bert-base-uncased\", return_dict=True)\n",
    "\n",
    "sequences = data[\"sequence\"]\n",
    "\n",
    "MAX_LENGTH = 128\n",
    "tokenized_data = sequences.apply(\n",
    "    (\n",
    "        lambda row: tokenizer.encode(\n",
    "            row, add_special_tokens=True, padding=\"max_length\", max_length=MAX_LENGTH\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "tokenized_data = tokenized_data.reset_index(drop=True)\n",
    "input_data = torch.tensor(tokenized_data)\n",
    "\n",
    "# Define the optimizer to be used to train the model\n",
    "dropout_rate = 0.3\n",
    "model = BERTSequenceImputer(dropout_rate=dropout_rate)\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5, eps=1e-8)\n",
    "\n",
    "# Train the model over a set number of epochs\n",
    "epochs = 4\n",
    "for epoch in range(epochs):\n",
    "    epoch_loss = 0\n",
    "    for i in range(len(input_data)):\n",
    "        # Reset gradients\n",
    "        model.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        input_ids = input_data[i, :]\n",
    "\n",
    "        attention_mask = [int(token_id.item() > 0) for token_id in input_ids]\n",
    "        attention_mask = torch.tensor(attention_mask).unsqueeze(0)\n",
    "\n",
    "        input_ids = input_ids.unsqueeze(0)\n",
    "\n",
    "        # input_ids[[0]][0][0].item()\n",
    "\n",
    "        # attention_mask = torch.tensor(attention_mask).unsqueeze(0)\n",
    "\n",
    "        y_pred = model.forward(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "        # Compute loss\n",
    "        y_true = torch.tensor([float(labels[i])])\n",
    "        loss_func = torch.nn.BCEWithLogitsLoss()\n",
    "        loss = loss_func(y_pred.view(-1), y_true.view(-1))\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # Update weights\n",
    "        optimizer.step()\n",
    "\n",
    "    # Print loss per epoch\n",
    "    print(f\"Epoch: {epoch+1}, Loss: {epoch_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = BertForMaskedLM.from_pretrained(\"bert-base-uncased\", return_dict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedded_seq = m.bert(\n",
    "    input_ids=input_ids, attention_mask=attention_mask, return_dict=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(embedded_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "do(embedded_seq[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '/Users/ask/Documents/GitHub/STA208/bin/python' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -p /Users/ask/Documents/GitHub/STA208 ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "%conda install -p /Users/ask/Documents/GitHub/STA208 numpy --update-deps --force-reinstall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
